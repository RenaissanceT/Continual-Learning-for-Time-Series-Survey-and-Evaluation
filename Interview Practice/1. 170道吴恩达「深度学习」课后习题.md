# 170道吴恩达「深度学习」课后习题 第一部分

## 第 1 题

**“人工智能是新电力”这个比喻指的是什么？**

**A.** 人工智能为我们的家庭和办公室的个人设备供电，类似于电力。

**B.** 通过“智能电网”，人工智能正在传递新一波的电力。

**C.** 人工智能在计算机上运行，因此由电力驱动，但它让计算机做以前不可能做的事情。

**D.** 与100年前开始的电力类似，人工智能正在改变多个行业。

#### 详解
“人工智能是新电力”这个比喻是用来说明人工智能技术对当今社会的巨大影响，就像100年前电力的普及彻底改变了许多行业一样。电力的广泛应用引发了各个行业的创新，改变了人们的生活方式。同样，人工智能正在对各个行业进行深刻的变革，它正在改变生产、医疗、金融、教育等多个领域的工作方式和效率。因此，人工智能被认为是现代社会的“新电力”。

#### 例子
100多年前，电力的出现彻底改变了制造业、农业和交通运输等多个领域。同样，现在人工智能也正在改变制造业中的自动化生产、医疗中的诊断与治疗、金融领域中的智能投顾等各个行业。

**正确答案选项：D**

---

## 第 2 题

**以下哪些是最近深度学习开始崛起的原因？（选2个答案）**

**A.** 我们拥有了更多的计算能力

**B.** 神经网络是一个崭新的领域。

**C.** 我们有了更多的数据。

**D.** 深度学习在诸如在线广告、语音识别和图像识别等重要应用方面取得了显著的改进。

#### 详解
深度学习的崛起主要归功于两个关键因素：首先，我们拥有了更多的计算能力（如GPU和TPU的出现），可以处理大规模的深度学习模型；其次，互联网和数字化时代的到来，产生了大量的数据，成为训练深度学习模型的重要资源。而神经网络并不是一个新的领域，它早在几十年前就被提出了；以及深度学习在某些应用领域的改进则是它崛起的结果而非原因。

#### 例子
- **计算能力**：现代GPU的并行处理能力使得训练复杂的深度学习模型在合理的时间内成为可能。
- **数据**：社交媒体、电子商务、传感器数据等都提供了海量的数据，这些数据为深度学习模型提供了丰富的学习素材。

**正确答案选项：A 和 C**

---

## 第 4 题

**当一个有经验的深度学习工程师处理一个新问题时，他们通常可以在第一次尝试时利用以前问题的洞察力来训练一个好的模型，而不需要在不同的模型中重复多次。**

**A.** 对  
**B.** 不对

#### 详解
通常情况下，经验丰富的深度学习工程师会利用以前问题的洞察力，但在实践中，可能需要多次尝试和调整来优化模型性能，因此不太可能在第一次就训练出一个完美的模型。 

#### 例子
在图像分类问题中，工程师可能尝试使用预训练的模型或相似问题的架构，但需要不断调整超参数来获得最佳效果。

**正确答案选项：B**

---

## 第 6 题

**用于猫识别的图像是“结构化”数据的一个例子，因为它在计算机中表示为结构化的数组。**

**A.** 对  
**B.** 不对

#### 详解
猫识别的图像数据虽然是以结构化数组的形式存储（如多维数组或矩阵），但从数据本质上讲，图像被认为是“非结构化”数据，因为它的像素信息本身没有明确的列或标签。

#### 例子
图像文件、音频文件、文本文件都属于非结构化数据，而数据库中的表格数据则属于结构化数据。

**正确答案选项：B**

---

## 第 7 题

**人口数据集包含不同城市人口、人均GDP、经济增长的统计数据，这是“非结构化”数据的一个例子，因为它包含来自不同来源的数据。**

**A.** 对  
**B.** 不对

#### 详解
人口数据集属于结构化数据，因为它通常以表格形式组织，具有固定的行和列，数据项明确。非结构化数据则是指没有预定义模型的数据，如文本、音频、图像等。

#### 例子
Excel表格中的人口统计数据属于结构化数据，而社交媒体帖子则属于非结构化数据。

**正确答案选项：B**

---

## 第 8 题

**为什么RNN（递归神经网络）被用于机器翻译，比如说将英语翻译成法语？（选出所有正确项）**

**A.** 它可以训练成一个有监督的学习问题  
**B.** 它比卷积神经网络（CNN）更强大  
**C.** 当输入/输出是一个序列（例如，一个单词序列）时适用  
**D.** RNN表示 想法->代码->实验->想法->... 的循环过程

#### 详解
RNN非常适合处理序列数据，因此在机器翻译中非常有效。机器翻译是一个有监督的学习问题，输入是一个序列（如英语单词序列），输出也是一个序列（如法语单词序列）。

#### 例子
将一段英语翻译成法语时，RNN能够通过处理前后单词的上下文关系来生成对应的法语翻译。

**正确答案选项：A 和 C**

---

## 第 10 题

**假设前一个问题中所描述的趋势是准确的（并且希望你的坐标轴标签正确），下列哪一个是正确的？（选出所有正确项）**

**A.** 增加训练集的大小通常不会影响算法的性能，而且可能会有很大帮助。  
**B.** 增加神经网络的规模通常不会影响算法的性能，而且可能会有很大帮助。  
**C.** 减小训练集的大小通常不会影响算法的性能，而且可能会有很大帮助。  
**D.** 减小神经网络的规模通常不会影响算法的性能，而且可能会有很大帮助。

#### 详解
通常，增加训练集的大小可以帮助模型学习得更好，尤其是当模型容量足够时。而增加神经网络的规模可以提高模型的表达能力，但并不是总是有效的。此外，减小训练集或网络规模通常会降低模型性能。

#### 例子
在图像识别中，更多的训练数据通常可以提高模型的准确性。

**正确答案选项：B**

---

## 第 11 题

**神经元计算什么？**

**A.** 神经元计算激活函数后,再计算线性函数（z=Wx+b）  
**B.** 神经元计算一个线性函数（z=Wx+b），然后接一个激活函数  
**C.** 神经元计算一个函数g，它线性地缩放输入x（Wx+b）  
**D.** 神经元先计算所有特征的平均值，然后将激活函数应用于输出

#### 详解
神经元首先计算输入的线性组合（z=Wx+b），然后通过激活函数对结果进行非线性变换。

#### 例子
一个简单的神经网络层的计算过程通常包括对输入的加权求和，然后应用激活函数，如ReLU或sigmoid。

**正确答案选项：B**

---

## 第 12 题

**以下哪一个是逻辑回归的损失函数？**

**A.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=|y^{(i)} - \hat{y}^{(i)}|$  
**B.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=max(0,y^{(i)} - \hat{y}^{(i)})$  
**C.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=|y^{(i)} - \hat{y}^{(i)}|^2$  
**D.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=-(y^{(i)}log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)}))$

#### 详解
逻辑回归的损失函数是交叉熵损失函数。

#### 例子
在二元分类问题中，交叉熵损失衡量了预测概率与真实标签的差异。

**正确答案选项：D**

---

## 第 13 题

**假设img是一个（32, 32, 3）数组，表示一个32x32图像，它有三个颜色通道：红色、绿色和蓝色。如何将其重塑为列向量？**

**A.** x = img.reshape((1, 32*32, 3))  
**B.** x = img.reshape((32*32*3, 1))  
**C.** x = img.reshape((3, 32*32))  
**D.** x = img.reshape((32*32, 3))

#### 详解
要将其重塑为列向量，我们需要将所有像素及其颜色通道展平成一个一维数组。

#### 例子
图像展平是常见的预处理步骤，将3D数据转为1D用于神经网络输入。

**正确答案选项：B**

---

## 第 16 题

**假设每个示例有$n_x$个输入特性，$X=[X^{(1)}, X^{(2)}, …, X^{(m)}]$。$X$的维数是多少？**

**A.** (m, 1)  
**B.** (1, m)  
**C.** ($n_x$, m)  
**D.** (m, $n_x$)

#### 详解
在机器学习中，$X$通常是一个矩阵，其中每一列代表一个训练示例，每一行代表一个特征。因此，如果我们有$m$个训练示例和$n_x$个输入特性，矩阵$X$的维度为($n_x$, m)。

#### 例子
例如，如果我们有100个训练示例，每个示例有20个特征，那么$X$的维度将是(20, 100)。

**正确答案选项：C**

---

## 第 21 题

**以下哪项是正确的？（选出所有正确项）**

**A.** $a^{ }$是第12层，第2个训练数据的激活向量  
**B.** $X$是一个矩阵，其中每个列是一个训练数据  
**C.** $a^{[2]}_4$是第2层，第4个训练数据的激活输出  
**D.** $a^{[2]}_4$是第2层，第4个神经元的激活输出  
**E.** $a^{[2]}$表示第2层的激活向量  
**F.** $a^{ }$是第2层，第12个数据的激活向量  
**G.** $X$是一个矩阵，其中每个行是一个训练数据

#### 详解
- **A** 错误，因为$a^{ }$实际上是第2层的第12个训练数据的激活值。
- **B** 正确，因为在机器学习中，$X$通常是一个矩阵，其中每一列代表一个训练数据。
- **C** 错误，因为$a^{[2]}_4$代表的是第2层中第4个神经元的激活输出，而不是第4个训练数据。
- **D** 正确，因为$a^{[2]}_4$表示第2层中第4个神经元的激活输出。
- **E** 正确，因为$a^{[2]}$表示第2层的激活向量。
- **F** 正确，因为$a^{ }$确实是第2层中第12个数据的激活向量。
- **G** 错误，因为$X$中的每一列代表一个训练数据，而不是每一行。

**正确答案选项：B, D, E, F**

---

## 第 22 题

**对于隐藏单元，tanh激活通常比sigmoid激活函数更有效，因为其输出的平均值接近于零，因此它可以更好地将数据集中到下一层。**

**A.** 对  
**B.** 不对

#### 详解
tanh激活函数的输出范围是[-1, 1]，其输出的平均值更接近于零，这使得数据更集中，能够更好地避免数据偏移问题，从而提高训练效率。相较之下，sigmoid的输出范围是[0, 1]，输出的均值偏离零，因此在某些情况下可能导致梯度消失问题。

#### 例子
在训练深度神经网络时，使用tanh激活通常会更快地收敛，尤其是对于隐藏层。

**正确答案选项：A**

---

## 第 23 题

**以下哪一个是$l$层的正向传播的正确矢量化实现，其中$1 \le l \le L$**

**A.**  
$Z^{[l]}=W^{[l]}A^{[l]}+b^{[l]}$  
$A^{[l+1]}=g^{[l]}(Z^{[l]})$

**B.**  
$Z^{[l]}=W^{[l]}A^{[l]}+b^{[l]}$  
$A^{[l+1]}=g^{[l+1]}(Z^{[l]})$

**C.**  
$Z^{[l]}=W^{[l-1]}A^{[l]}+b^{[l]}$  
$A^{[l]}=g^{[l]}(Z^{[l]})$

**D.**  
$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$  
$A^{[l]}=g^{[l]}(Z^{[l]})$

#### 详解
正向传播的矢量化实现通常是$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$，然后通过激活函数$g^{[l]}$得到$A^{[l]}=g^{[l]}(Z^{[l]})$。$A^{[l]}$表示第$l$层的激活值，$W^{[l]}$和$b^{[l]}$是第$l$层的权重和偏置。

#### 例子
如果$A^{[l-1]}$是上一层的输出，那么第$l$层计算会使用该值来得到本层的激活输出。

**正确答案选项：D**

---

## 第 24 题

**您正在构建一个用于识别黄瓜（y=1）与西瓜（y=0）的二进制分类器。对于输出层，您建议使用哪一个激活函数？**

**A.** ReLU  
**B.** Leaky ReLU  
**C.** sigmoid  
**D.** tanh

#### 详解
对于二进制分类问题，输出层的激活函数通常选择sigmoid函数，因为它可以将输出值压缩到[0, 1]的范围，方便将其解释为概率。ReLU和Leaky ReLU通常用于隐藏层的激活，而tanh虽然可以用于分类，但由于其范围是[-1, 1]，不如sigmoid在二元分类中常用。

#### 例子
在二元分类问题中，如图像分类、垃圾邮件检测等，sigmoid函数能够输出一个介于0和1之间的值，代表预测为类别1的概率。

**正确答案选项：C**

---

## 第 26 题

**假设你已经建立了一个神经网络。您决定将权重和偏差初始化为零。以下哪项陈述是正确的？（选出所有正确项）**

**A.** 第一隐藏层中的每个神经元将执行相同的计算。因此，即使在梯度下降的多次迭代之后，层中的每个神经元将执行与其他神经元相同的计算。

**B.** 第一隐层中的每个神经元在第一次迭代中执行相同的计算。但是在梯度下降的一次迭代之后，他们将学会计算不同的东西，因为我们已经“破坏了对称性”。

**C.** 第一个隐藏层中的每个神经元将执行相同的计算，但不同层中的神经元执行不同的计算，因此我们完成了课堂上所描述的“对称性破坏”。

**D.** 即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算，因此，它们的参数会以自己的方式不断演化。

#### 详解
如果将权重初始化为零，那么所有神经元将执行相同的计算，并且会计算出相同的梯度。这种现象称为“对称性问题”，它会导致所有神经元在训练中无法学习到不同的特征。因此，A 是正确的描述。选项 B、C、D 都是错误的，因为对称性没有被破坏，所有神经元都会在整个训练过程中保持相同的计算。

#### 例子
在训练神经网络时，如果初始化所有权重为零，隐藏层的神经元会学习到完全相同的内容，无法实现网络的多样性和学习能力。

**正确答案选项：A**

---

## 第 27 题

**逻辑回归的权重w应该随机初始化，而不是全部初始化为全部零，否则，逻辑回归将无法学习有用的决策边界，因为它将无法“打破对称”**

**A.** 对  
**B.** 不对

#### 详解
在逻辑回归中，虽然只有一个输出神经元，但随机初始化权重仍然是一个好习惯。将所有权重初始化为零将导致模型永远无法学到任何有效的信息，因为所有输入的影响将完全相同，无法破坏对称性。

#### 例子
在训练逻辑回归时，如果将权重初始化为零，梯度下降不会收敛到有意义的权重值。

**正确答案选项：A**

---

## 第 28 题

**你已经为所有隐藏的单位建立了一个使用tanh激活的网络。使用np.random.randn(…, …)*1000将权重初始化为相对较大的值。会发生什么？**

**A.** 没关系。只要随机初始化权重，梯度下降不受权重大小的影响。  
**B.** 这将导致tanh的输入也非常大，从而导致梯度也变大。因此，你必须将$\alpha$设置得非常小，以防止发散；这将减慢学习速度。  
**C.** 这将导致tanh的输入也非常大，导致单元被“高度激活”。与权重从小值开始相比，加快了学习速度。  
**D.** 这将导致tanh的输入也非常大，从而导致梯度接近于零。因此，优化算法将变得缓慢。

#### 详解
如果权重初始化过大，tanh函数的输入值会非常大，导致其输出接近-1或1。在这种情况下，tanh函数的梯度几乎为零，这会导致梯度消失问题，导致优化算法变得非常缓慢。

#### 例子
在深度神经网络中，如果权重初始化过大，激活函数会进入“饱和区”，无法有效地传播梯度。

**正确答案选项：D**

---

## 第 30 题

**在和上一问相同的网络中，$Z^{[1]}$ 和 $A^{[1]}$的维度是多少？**

**A.** $Z^{[1]}$ 和 $A^{[1]}$是(4, 1)  
**B.** $Z^{[1]}$ 和 $A^{[1]}$是(1, 4)  
**C.** $Z^{[1]}$ 和 $A^{[1]}$是(4, m)  
**D.** $Z^{[1]}$ 和 $A^{[1]}$是(4, 2)

#### 详解
通常情况下，$Z^{[1]}$和$A^{[1]}$的维度取决于当前层的神经元数量和输入样本数量。如果第1层有4个神经元，且有$m$个训练样本，那么$Z^{[1]}$和$A^{[1]}$的维度应为(4, m)。

#### 例子
对于一个含有5个神经元和3个训练样本的隐藏层，$Z$和$A$的维度将为(5, 3)。

**正确答案选项：C**

----

## 第 31 题

**在我们的前向传播和后向传播实现中使用的“缓存”是什么？**

**A.** 它用于在训练期间缓存成本函数的中间值。  
**B.** 我们用它将在正向传播过程中计算的变量传递到相应的反向传播步骤。它包含了反向传播计算导数的有用值。  
**C.** 它用于跟踪我们正在搜索的超参数，以加快计算速度。  
**D.** 我们用它将反向传播过程中计算的变量传递到相应的正向传播步骤。它包含用于计算正向传播的激活的有用值。

#### 详解
在深度学习中，缓存用于保存前向传播中的中间值，这些中间值在反向传播时会用到。它帮助我们在反向传播中计算梯度。

**正确答案选项：B**

---

## 第 32 题

**以下哪些是“超参数”？(选出所有正确项)**

**A.** 隐藏层规模$n^{[l]}$  
**B.** 神经网络的层数$L$  
**C.** 激活向量$a^{[l]}$  
**D.** 权重矩阵$W^{[l]}$  
**E.** 学习率$\alpha$  
**F.** 迭代次数  
**G.** 偏置向量$b^{[l]}$

#### 详解
超参数是由用户手动设定的参数，通常包括隐藏层的规模、神经网络的层数、学习率和迭代次数等。权重矩阵和偏置是训练中学到的参数，因此它们不是超参数。

**正确答案选项：A, B, E, F**

---

## 第 33 题

**下列哪个说法是正确的？**

**A.** 神经网络的更深层通常比前面的层计算更复杂的特征  
**B.** 神经网络的前面的层通常比更深层计算更复杂的特性

#### 详解
神经网络的浅层通常提取简单的特征，如边缘或颜色等，而更深的层则提取更复杂的特征，如形状或物体的高级特征。

**正确答案选项：A**

---

## 第 34 题

**向量化允许您在L层神经网络中计算前向传播时，不需要在层l = 1, 2, …, L间显式的使用for循环（或任何其他显式迭代循环）**

**A.** 对  
**B.** 不对

#### 详解
向量化通过并行操作加速了计算，但前向传播仍然需要按照层次顺序进行，即使是向量化的实现也无法完全避免显式的层间迭代。

**正确答案选项：B**

---

## 第 37 题

**在前向传播期间，在层$l$的前向传播函数中，您需要知道层$l$中的激活函数（Sigmoid，tanh，ReLU等）是什么。在反向传播期间，相应的反向传播函数也需要知道第$l$层的激活函数是什么，因为梯度是根据它来计算的**

**A.** 对  
**B.** 不对

#### 详解
在反向传播中，梯度的计算依赖于前向传播中使用的激活函数的导数，因此需要知道具体使用了什么激活函数。

**正确答案选项：A**

---

## 第 38 题

**有一些函数具有以下特性：  
(i) 当使用浅网络计算时，需要一个大网络（我们通过网络中的逻辑门数量来度量大小），但是  
(ii) 当使用深网络来计算时，我们只需要一个指数级小的网络**

**A.** 对  
**B.** 不对

#### 详解
这是深度网络优于浅网络的一个优势：某些函数可以用更少的参数通过深度网络计算，而浅网络则需要更多的资源来计算相同的结果。

**正确答案选项：A**

---

## 第 40 题

**前面的问题使用了一个特定的网络，一般情况下，层$l$的权重矩阵$W^{[l]}$的维数是多少?**

**A.** $W^{[l]}$ 的形状是 $(n^{[l]},n^{[l-1]})$  
**B.** $W^{[l]}$ 的形状是 $(n^{[l-1]},n^{[l]})$  
**C.** $W^{[l]}$ 的形状是 $(n^{[l+1]},n^{[l]})$  
**D.** $W^{[l]}$ 的形状是 $(n^{[l]},n^{[l+1]})$

#### 详解
权重矩阵$W^{[l]}$的行数为该层的神经元个数，列数为上一层的神经元个数，因此维度为$(n^{[l]},n^{[l-1]})$。

**正确答案选项：A**

---

## 第 41 题

**如果你有10,000,000个例子，你会如何划分训练/开发/测试集？**

**A.** 33%训练，33%开发，33%测试  
**B.** 60%训练，20%开发，20%测试  
**C.** 98%训练，1%开发，1%测试

#### 详解
对于大数据集，如10,000,000个示例，建议将大部分数据用于训练，较少的数据用于开发和测试。因此98%用于训练、1%用于开发、1%用于测试是最合理的。

**正确答案选项：C**

---

## 第 42 题

**开发和测试集应该：**

**A.** 来自同一分布  
**B.** 来自不同分布  
**C.** 完全相同（一样的(x, y)对）  
**D.** 数据数量应该相同

#### 详解
开发集和测试集应该来自相同的分布，以确保模型在开发和测试阶段的表现具有一致性和可比性。

**正确答案选项：A**

---

## 第 43 题

**如果你的神经网络方差很高，下列哪个尝试是可能解决问题的？**

**A.** 添加正则项  
**B.** 获取更多测试数据  
**C.** 增加每个隐藏层的神经元数量  
**D.** 用更深的神经网络  
**E.** 用更多的训练数据

#### 详解
方差高意味着模型过拟合，可以通过添加正则化或增加训练数据来缓解。增加神经元或层数会加剧过拟合。

**正确答案选项：A, E**

---

## 第 44 题

**你正在为苹果，香蕉和橘子制作分类器。 假设您的分类器在训练集上有0.5％的错误，以及开发集上有7％的错误。 以下哪项尝试是有希望改善你的分类器的分类效果的？**

**A.** 增大正则化参数$\lambda$  
**B.** 减小正则化参数$\lambda$  
**C.** 获取更多训练数据  
**D.** 用更大的神经网络

#### 详解
模型在开发集上的表现较差，表明其出现了过拟合问题，增加正则化（增大$\lambda$）有助于解决这一问题。

**正确答案选项：A**

---

## 第 45 题

**什么是权重衰减？**

**A.** 正则化技术（例如L2正则化）导致梯度下降在每次迭代时权重收缩  
**B.** 在训练过程中逐渐降低学习率的过程  
**C.** 如果神经网络是在噪声数据下训练的，那么神经网络的权值会逐渐损坏  
**D.** 通过对权重值设置上限来避免梯度消失的技术

#### 详解
权重衰减是L2正则化的另一个名称，它通过在损失函数中加入权重平方项，使得权重逐渐减小，防止模型过拟合。

**正确答案选项：A**

---

## 第 46 题

**当你增大正则化的超参数$\lambda$时会发生什么？**

**A.** 权重变小（接近0）  
**B.** 重量变大（远离0）  
**C.** 2倍的$\lambda$导致2倍的权重  
**D.** 每次迭代，梯度下降采取更大的步距（与$\lambda$成正比）

#### 详解
增加正则化参数$\lambda$会导致权重的绝对值变小，以抑制模型的过拟合。

**正确答案选项：A**

---

## 第 47 题

**在测试时候使用dropout：**

**A.** 不随机关闭神经元，但保留1/keep_prob因子  
**B.** 随机关闭神经元，保留1/keep_prob因子  
**C.** 随机关闭神经元，但不保留1/keep_prob因子  
**D.** 不随机关闭神经元，也不保留1/keep_prob因子

#### 详解
在测试时，dropout不再应用，所有神经元都参与计算，因此不随机关闭任何神经元。

**正确答案选项：D**

---

## 第 48 题

**将参数 keep_prob 从（比如说）0.5增加到0.6可能会导致以下情况（选出所有正确项）：**

**A.** 正则化效应被增强  
**B.** 正则化效应被减弱  
**C.** 训练集的误差会增加  
**D.** 训练集的误差会减小

#### 详解
keep_prob表示在每次迭代中保留神经元的概率。将keep_prob从0.5增加到0.6意味着保留更多的神经元，减少了dropout的正则化效应。因此，正则化效应被减弱，并且可能导致训练集误差的减小（因为模型在训练时的拟合能力增强）。

**正确答案选项：B, D**

---

## 第 49 题

**以下哪些技术可用于减少方差（减少过拟合）？（选出所有正确项）**

**A.** 梯度消失  
**B.** 数据扩充  
**C.** Dropout  
**D.** 梯度检查  
**E.** Xavier初始化  
**F.** L2正则化  
**G.** 梯度爆炸

#### 详解
减少方差的技术包括数据扩充（增加训练数据的多样性）、Dropout（防止神经元过度依赖特定特征）、以及L2正则化（减少权重值的大小，从而避免过拟合）。梯度消失和爆炸是与梯度计算相关的问题，而梯度检查和Xavier初始化并不是用于减少方差的方法。

**正确答案选项：B, C, F**

---

## 第 50 题

**为什么要对输入$x$进行归一化？**

**A.** 让参数初始化更快  
**B.** 让代价函数更快地优化  
**C.** 更容易做数据可视化  
**D.** 是另一种正则化——有助减少方差

#### 详解
归一化输入数据可以使模型更快地收敛，因为它使得特征值位于一个相似的尺度范围内，减少了梯度下降时的震荡。

**正确答案选项：B**

---

## 第 51 题

**当输入从第8个mini-batch的第7个的例子的时候，你会用哪种符号表示第3层的激活？**

**A.** $a^{[3]\{8\}(7)}$  
**B.** $a^{[8]\{7\}(3)}$  
**C.** $a^{[8]\{3\}(7)}$  
**D.** $a^{[3]\{7\}(8)}$

#### 详解
激活表示的符号$a^{[3]\{8\}(7)}$表示的是第3层，第8个mini-batch，第7个样本的激活值。

**正确答案选项：A**

---

## 第 52 题

**关于mini-batch的说法哪个是正确的？**

**A.** mini-batch迭代一次（计算1个mini-batch），要比批量梯度下降迭代一次快  
**B.** 用mini-batch训练完整个数据集一次，要比批量梯度下降训练完整个数据集一次快  
**C.** 在不同的mini-batch下，不需要显式地进行循环，就可以实现mini-batch梯度下降，从而使算法同时处理所有的数据（矢量化）

#### 详解
mini-batch的每次迭代比批量梯度下降更快，因为它在计算中只处理部分数据。经过多次迭代后，整个数据集会被完整地训练，但每一轮的训练时间更短。

**正确答案选项：A**

---

## 第 53 题

**为什么最好的mini-batch的大小通常不是1也不是m，而是介于两者之间？**

**A.** 如果mini-batch的大小是1，那么在你取得进展前，你需要遍历整个训练集  
**B.** 如果mini-batch的大小是m，就会变成批量梯度下降。在你取得进展前，你需要遍历整个训练集  
**C.** 如果mini-batch的大小是1，那么你将失去mini-batch将数据矢量化带来的的好处  
**D.** 如果mini-batch的大小是m，就会变成随机梯度下降，而这样做经常会比mini-batch慢

#### 详解
mini-batch的大小介于1和m之间，是因为它结合了随机梯度下降和批量梯度下降的优势，既可以利用并行计算的效率，又不会使训练过于缓慢。

**正确答案选项：B, C**

---

## 第 55 题

**假设一月的前三天卡萨布兰卡的气温是一样的：  
一月第一天: $\theta_1 = 10$  
一月第二天: $\theta_2 = 10$**

**假设您使用$\beta = 0.5$的指数加权平均来跟踪温度：$v_0=0,v_t=\beta v_{t-1}+(1-\beta)\theta_t$。如果$v_2$是在没有偏差修正的情况下计算第2天后的值，并且$v_2^{corrected}$是您使用偏差修正计算的值。 这些下面的值是正确的是？**

**A.** $v_2=10,v_2^{corrected}=10$  
**B.** $v_2=10,v_2^{corrected}=7.5$  
**C.** $v_2=7.5,v_2^{corrected}=7.5$  
**D.** $v_2=7.5,v_2^{corrected}=10$

#### 详解
未经过偏差修正的$v_2 = 0.5 \times 10 + 0.5 \times 10 = 7.5$。经过偏差修正的$v_2^{corrected} = v_2 / (1 - \beta^2) = 7.5 / 0.5 = 10$。

**正确答案选项：D**

---

## 第 56 题

**下面哪一个不是比较好的学习率衰减方法？**

**A.** $\alpha = \frac{1}{1+2*t}\alpha_0$  
**B.** $\alpha=\frac{1}{\sqrt{t}}\alpha_0$  
**C.** $\alpha=0.95^t\alpha_0$  
**D.** $\alpha=e^t\alpha_0$

#### 详解
在学习率衰减中，D的指数增长会导致学习率过快增大，这是不合理的，因此不是好的方法。

**正确答案选项：D**

---

## 第 59 题

**假设在一个深度学习网络中，批量梯度下降花费了大量时间时来找到一组参数值，使成本函数$J(W^{[1]},b^{[1]},…,W^{[L]},b^{[L]})$小。以下哪些方法可以帮助找到$J$值较小的参数值？**

**A.** 令所有权重值初始化为0  
**B.** 尝试调整学习率  
**C.** 尝试mini-batch梯度下降  
**D.** 尝试对权重进行更好的随机初始化  
**E.** 尝试使用 Adam 算法

#### 详解
调整学习率、使用mini-batch梯度下降、进行更好的权重初始化和使用Adam优化算法都是常见的加速训练的方法。初始化权重为0不是有效的方法。

**正确答案选项：B, C, D, E**

---

## 第 60 题

**关于Adam算法，下列哪一个陈述是错误的？**

**A.** Adam结合了Rmsprop和动量的优点  
**B.** Adam中的学习率超参数$\alpha$通常需要调整  
**C.** 我们经常使用超参数的“默认”值$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$  
**D.** Adam应该用于批梯度计算，而不是用于mini-batch

#### 详解
Adam可以用于mini-batch梯度下降，并且通常用于此目的。因此，D是错误的陈述。

**正确答案选项：D**

---

## 第 61 题

**如果在大量的超参数中搜索最佳的参数值，那么应该尝试在网格中搜索而不是使用随机值，以便更系统的搜索，而不是依靠运气，请问这句话是正确的吗？**

**A.** 对  
**B.** 不对

#### 详解
在高维空间中，随机搜索通常比网格搜索更有效，因为随机搜索可以探索更多的组合，而网格搜索在维度较高时容易错过一些潜在的最佳参数值。

**正确答案选项：B**

---

## 第 62 题

**每个超参数如果设置得不好，都会对训练产生巨大的负面影响，因此所有的超参数都要调整好，请问这是正确的吗？**

**A.** 对  
**B.** 不对

#### 详解
在实践中，有些超参数对模型的性能影响较大，而其他一些超参数影响较小。通常，优先调整最重要的超参数，例如学习率和正则化参数。

**正确答案选项：B**

---

## 第 63 题

**在超参数搜索过程中，你尝试只照顾一个模型（使用熊猫策略）还是一起训练大量的模型（鱼子酱策略）在很大程度上取决于：**

**A.** 是否使用批量（batch）或小批量优化（mini-batch optimization）  
**B.** 神经网络中局部最小值（鞍点）的存在性  
**C.** 在你能力范围内，你能够拥有多大的计算能力（博主注：就是高性能电脑和低性能电脑的区别）  
**D.** 需要调整的超参数的数量

#### 详解
你的计算能力决定了你是一次同时训练多个模型（鱼子酱策略），还是一次训练一个模型（熊猫策略）。如果计算资源有限，只能使用熊猫策略。

**正确答案选项：C**

---

## 第 65 题

**找到好的超参数的值是非常耗时的，所以通常情况下你应该在项目开始时做一次，并尝试找到非常好的超参数，这样你就不必再次重新调整它们。请问这正确吗？**

**A.** 对  
**B.** 不对

#### 详解
在实践中，由于数据、模型和目标可能不断变化，超参数通常需要多次调整。找到最佳的超参数并不是一劳永逸的过程。

**正确答案选项：B**

---

## 第 66 题

**在视频中介绍的批量标准化中，如果将其应用于神经网络的第$l$层，您应该对谁进行标准化？**

**A.** $z^{[l]}$  
**B.** $W^{[l]}$  
**C.** $a^{[l]}$  
**D.** $b^{[l]}$

#### 详解
批量标准化通常应用于$z^{[l]}$，即在线性变换$W^{[l]}a^{[l-1]} + b^{[l]}$之后，对其进行标准化。

**正确答案选项：A**

---

## 第 67 题

**在标准化公式$z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$，为什么要使用epsilon（ϵ）？**

**A.** 为了更准确地标准化  
**B.** 为了避免除零操作  
**C.** 为了加速收敛  
**D.** 防止$\mu$太小

#### 详解
epsilon（ϵ）是一个小常数，通常用于避免除零的操作，确保计算稳定。

**正确答案选项：B**

---

## 第 68 题

**批标准化中关于$\gamma$和$\beta$的以下哪些陈述是正确的？**

**A.** 对于每个层，有一个全局值$\gamma \in \mathbb{R}$和一个全局值$\beta \in \mathbb{R}$，适用于该层中的所有隐藏单元。  
**B.** $\gamma$和$\beta$是算法的超参数，我们通过随机采样进行调整  
**C.** 它们确定了给定层的线性变量$z^{[l]}$的均值和方差  
**D.** 最佳值是$\gamma=\sqrt{\sigma^2+\epsilon},\beta=\mu$  
**E.** 它们可以用Adam、动量的梯度下降或RMSprop，而不仅仅是用梯度下降来学习

#### 详解
在批标准化中，$\gamma$和$\beta$是参数，它们通过优化算法（如Adam、RMSprop）进行学习，并不是通过随机采样或人为设置的。它们用于调整标准化后的均值和方差。

**正确答案选项：E**

---

## 第 69 题

**在训练了具有批标准化的神经网络之后，在用新样本评估神经网络的时候，您应该：**

**A.** 如果你在256个例子的mini-batch上实现了批标准化，那么如果你要在一个测试例子上进行评估，你应该将这个例子重复256次，这样你就可以使用和训练时大小相同的mini-batch进行预测。  
**B.** 使用最新的mini-batch的$\mu$和$\sigma^2$值来执行所需的标准化  
**C.** 跳过用$\mu$和$\sigma^2$值标准化的步骤，因为一个例子不需要标准化  
**D.** 执行所需的标准化，使用在训练期间，通过指数加权平均值得出的$\mu$和$\sigma^2$

#### 详解
在训练过程中，我们会对批次中的数据进行标准化。在测试时，我们使用训练期间得到的均值和方差的指数加权平均值对数据进行标准化。

**正确答案选项：D**

---

## 第 70 题

**关于深度学习编程框架的这些陈述中，哪一个是正确的？（选出所有正确项）**

**A.** 即使一个项目目前是开源的，项目的良好管理有助于确保它即使在长期内仍然保持开放，而不是仅仅为了一个公司而关闭或修改。  
**B.** 通过编程框架，您可以使用比低级语言（如Python）更少的代码来编写深度学习算法。  
**C.** 深度学习编程框架的运行需要基于云的机器。

#### 详解
深度学习编程框架（如TensorFlow、PyTorch）简化了算法的实现，减少了所需的代码量，并且这些框架可以在本地机器上运行，不一定需要基于云的机器。此外，良好的项目管理对于保持项目的开源和长期可用性非常重要。

**正确答案选项：A, B**

---





















