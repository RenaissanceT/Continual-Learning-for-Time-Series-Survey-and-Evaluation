# 170道吴恩达「深度学习」课后习题 第一部分

## 第 1 题

**“人工智能是新电力”这个比喻指的是什么？**

**A.** 人工智能为我们的家庭和办公室的个人设备供电，类似于电力。

**B.** 通过“智能电网”，人工智能正在传递新一波的电力。

**C.** 人工智能在计算机上运行，因此由电力驱动，但它让计算机做以前不可能做的事情。

**D.** 与100年前开始的电力类似，人工智能正在改变多个行业。

#### 详解
“人工智能是新电力”这个比喻是用来说明人工智能技术对当今社会的巨大影响，就像100年前电力的普及彻底改变了许多行业一样。电力的广泛应用引发了各个行业的创新，改变了人们的生活方式。同样，人工智能正在对各个行业进行深刻的变革，它正在改变生产、医疗、金融、教育等多个领域的工作方式和效率。因此，人工智能被认为是现代社会的“新电力”。

#### 例子
100多年前，电力的出现彻底改变了制造业、农业和交通运输等多个领域。同样，现在人工智能也正在改变制造业中的自动化生产、医疗中的诊断与治疗、金融领域中的智能投顾等各个行业。

**正确答案选项：D**

---

## 第 2 题

**以下哪些是最近深度学习开始崛起的原因？（选2个答案）**

**A.** 我们拥有了更多的计算能力

**B.** 神经网络是一个崭新的领域。

**C.** 我们有了更多的数据。

**D.** 深度学习在诸如在线广告、语音识别和图像识别等重要应用方面取得了显著的改进。

#### 详解
深度学习的崛起主要归功于两个关键因素：首先，我们拥有了更多的计算能力（如GPU和TPU的出现），可以处理大规模的深度学习模型；其次，互联网和数字化时代的到来，产生了大量的数据，成为训练深度学习模型的重要资源。而神经网络并不是一个新的领域，它早在几十年前就被提出了；以及深度学习在某些应用领域的改进则是它崛起的结果而非原因。

#### 例子
- **计算能力**：现代GPU的并行处理能力使得训练复杂的深度学习模型在合理的时间内成为可能。
- **数据**：社交媒体、电子商务、传感器数据等都提供了海量的数据，这些数据为深度学习模型提供了丰富的学习素材。

**正确答案选项：A 和 C**

---

## 第 4 题

**当一个有经验的深度学习工程师处理一个新问题时，他们通常可以在第一次尝试时利用以前问题的洞察力来训练一个好的模型，而不需要在不同的模型中重复多次。**

**A.** 对  
**B.** 不对

#### 详解
通常情况下，经验丰富的深度学习工程师会利用以前问题的洞察力，但在实践中，可能需要多次尝试和调整来优化模型性能，因此不太可能在第一次就训练出一个完美的模型。 

#### 例子
在图像分类问题中，工程师可能尝试使用预训练的模型或相似问题的架构，但需要不断调整超参数来获得最佳效果。

**正确答案选项：B**

---

## 第 6 题

**用于猫识别的图像是“结构化”数据的一个例子，因为它在计算机中表示为结构化的数组。**

**A.** 对  
**B.** 不对

#### 详解
猫识别的图像数据虽然是以结构化数组的形式存储（如多维数组或矩阵），但从数据本质上讲，图像被认为是“非结构化”数据，因为它的像素信息本身没有明确的列或标签。

#### 例子
图像文件、音频文件、文本文件都属于非结构化数据，而数据库中的表格数据则属于结构化数据。

**正确答案选项：B**

---

## 第 7 题

**人口数据集包含不同城市人口、人均GDP、经济增长的统计数据，这是“非结构化”数据的一个例子，因为它包含来自不同来源的数据。**

**A.** 对  
**B.** 不对

#### 详解
人口数据集属于结构化数据，因为它通常以表格形式组织，具有固定的行和列，数据项明确。非结构化数据则是指没有预定义模型的数据，如文本、音频、图像等。

#### 例子
Excel表格中的人口统计数据属于结构化数据，而社交媒体帖子则属于非结构化数据。

**正确答案选项：B**

---

## 第 8 题

**为什么RNN（递归神经网络）被用于机器翻译，比如说将英语翻译成法语？（选出所有正确项）**

**A.** 它可以训练成一个有监督的学习问题  
**B.** 它比卷积神经网络（CNN）更强大  
**C.** 当输入/输出是一个序列（例如，一个单词序列）时适用  
**D.** RNN表示 想法->代码->实验->想法->... 的循环过程

#### 详解
RNN非常适合处理序列数据，因此在机器翻译中非常有效。机器翻译是一个有监督的学习问题，输入是一个序列（如英语单词序列），输出也是一个序列（如法语单词序列）。

#### 例子
将一段英语翻译成法语时，RNN能够通过处理前后单词的上下文关系来生成对应的法语翻译。

**正确答案选项：A 和 C**

---

## 第 10 题

**假设前一个问题中所描述的趋势是准确的（并且希望你的坐标轴标签正确），下列哪一个是正确的？（选出所有正确项）**

**A.** 增加训练集的大小通常不会影响算法的性能，而且可能会有很大帮助。  
**B.** 增加神经网络的规模通常不会影响算法的性能，而且可能会有很大帮助。  
**C.** 减小训练集的大小通常不会影响算法的性能，而且可能会有很大帮助。  
**D.** 减小神经网络的规模通常不会影响算法的性能，而且可能会有很大帮助。

#### 详解
通常，增加训练集的大小可以帮助模型学习得更好，尤其是当模型容量足够时。而增加神经网络的规模可以提高模型的表达能力，但并不是总是有效的。此外，减小训练集或网络规模通常会降低模型性能。

#### 例子
在图像识别中，更多的训练数据通常可以提高模型的准确性。

**正确答案选项：B**

---

## 第 11 题

**神经元计算什么？**

**A.** 神经元计算激活函数后,再计算线性函数（z=Wx+b）  
**B.** 神经元计算一个线性函数（z=Wx+b），然后接一个激活函数  
**C.** 神经元计算一个函数g，它线性地缩放输入x（Wx+b）  
**D.** 神经元先计算所有特征的平均值，然后将激活函数应用于输出

#### 详解
神经元首先计算输入的线性组合（z=Wx+b），然后通过激活函数对结果进行非线性变换。

#### 例子
一个简单的神经网络层的计算过程通常包括对输入的加权求和，然后应用激活函数，如ReLU或sigmoid。

**正确答案选项：B**

---

## 第 12 题

**以下哪一个是逻辑回归的损失函数？**

**A.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=|y^{(i)} - \hat{y}^{(i)}|$  
**B.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=max(0,y^{(i)} - \hat{y}^{(i)})$  
**C.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=|y^{(i)} - \hat{y}^{(i)}|^2$  
**D.** $L^{(i)}(\hat{y}^{(i)},y^{(i)})=-(y^{(i)}log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)}))$

#### 详解
逻辑回归的损失函数是交叉熵损失函数。

#### 例子
在二元分类问题中，交叉熵损失衡量了预测概率与真实标签的差异。

**正确答案选项：D**

---

## 第 13 题

**假设img是一个（32, 32, 3）数组，表示一个32x32图像，它有三个颜色通道：红色、绿色和蓝色。如何将其重塑为列向量？**

**A.** x = img.reshape((1, 32*32, 3))  
**B.** x = img.reshape((32*32*3, 1))  
**C.** x = img.reshape((3, 32*32))  
**D.** x = img.reshape((32*32, 3))

#### 详解
要将其重塑为列向量，我们需要将所有像素及其颜色通道展平成一个一维数组。

#### 例子
图像展平是常见的预处理步骤，将3D数据转为1D用于神经网络输入。

**正确答案选项：B**

---

## 第 16 题

**假设每个示例有$n_x$个输入特性，$X=[X^{(1)}, X^{(2)}, …, X^{(m)}]$。$X$的维数是多少？**

**A.** (m, 1)  
**B.** (1, m)  
**C.** ($n_x$, m)  
**D.** (m, $n_x$)

#### 详解
在机器学习中，$X$通常是一个矩阵，其中每一列代表一个训练示例，每一行代表一个特征。因此，如果我们有$m$个训练示例和$n_x$个输入特性，矩阵$X$的维度为($n_x$, m)。

#### 例子
例如，如果我们有100个训练示例，每个示例有20个特征，那么$X$的维度将是(20, 100)。

**正确答案选项：C**

---

## 第 21 题

**以下哪项是正确的？（选出所有正确项）**

**A.** $a^{ }$是第12层，第2个训练数据的激活向量  
**B.** $X$是一个矩阵，其中每个列是一个训练数据  
**C.** $a^{[2]}_4$是第2层，第4个训练数据的激活输出  
**D.** $a^{[2]}_4$是第2层，第4个神经元的激活输出  
**E.** $a^{[2]}$表示第2层的激活向量  
**F.** $a^{ }$是第2层，第12个数据的激活向量  
**G.** $X$是一个矩阵，其中每个行是一个训练数据

#### 详解
- **A** 错误，因为$a^{ }$实际上是第2层的第12个训练数据的激活值。
- **B** 正确，因为在机器学习中，$X$通常是一个矩阵，其中每一列代表一个训练数据。
- **C** 错误，因为$a^{[2]}_4$代表的是第2层中第4个神经元的激活输出，而不是第4个训练数据。
- **D** 正确，因为$a^{[2]}_4$表示第2层中第4个神经元的激活输出。
- **E** 正确，因为$a^{[2]}$表示第2层的激活向量。
- **F** 正确，因为$a^{ }$确实是第2层中第12个数据的激活向量。
- **G** 错误，因为$X$中的每一列代表一个训练数据，而不是每一行。

**正确答案选项：B, D, E, F**

---

## 第 22 题

**对于隐藏单元，tanh激活通常比sigmoid激活函数更有效，因为其输出的平均值接近于零，因此它可以更好地将数据集中到下一层。**

**A.** 对  
**B.** 不对

#### 详解
tanh激活函数的输出范围是[-1, 1]，其输出的平均值更接近于零，这使得数据更集中，能够更好地避免数据偏移问题，从而提高训练效率。相较之下，sigmoid的输出范围是[0, 1]，输出的均值偏离零，因此在某些情况下可能导致梯度消失问题。

#### 例子
在训练深度神经网络时，使用tanh激活通常会更快地收敛，尤其是对于隐藏层。

**正确答案选项：A**

---

## 第 23 题

**以下哪一个是$l$层的正向传播的正确矢量化实现，其中$1 \le l \le L$**

**A.**  
$Z^{[l]}=W^{[l]}A^{[l]}+b^{[l]}$  
$A^{[l+1]}=g^{[l]}(Z^{[l]})$

**B.**  
$Z^{[l]}=W^{[l]}A^{[l]}+b^{[l]}$  
$A^{[l+1]}=g^{[l+1]}(Z^{[l]})$

**C.**  
$Z^{[l]}=W^{[l-1]}A^{[l]}+b^{[l]}$  
$A^{[l]}=g^{[l]}(Z^{[l]})$

**D.**  
$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$  
$A^{[l]}=g^{[l]}(Z^{[l]})$

#### 详解
正向传播的矢量化实现通常是$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$，然后通过激活函数$g^{[l]}$得到$A^{[l]}=g^{[l]}(Z^{[l]})$。$A^{[l]}$表示第$l$层的激活值，$W^{[l]}$和$b^{[l]}$是第$l$层的权重和偏置。

#### 例子
如果$A^{[l-1]}$是上一层的输出，那么第$l$层计算会使用该值来得到本层的激活输出。

**正确答案选项：D**

---

## 第 24 题

**您正在构建一个用于识别黄瓜（y=1）与西瓜（y=0）的二进制分类器。对于输出层，您建议使用哪一个激活函数？**

**A.** ReLU  
**B.** Leaky ReLU  
**C.** sigmoid  
**D.** tanh

#### 详解
对于二进制分类问题，输出层的激活函数通常选择sigmoid函数，因为它可以将输出值压缩到[0, 1]的范围，方便将其解释为概率。ReLU和Leaky ReLU通常用于隐藏层的激活，而tanh虽然可以用于分类，但由于其范围是[-1, 1]，不如sigmoid在二元分类中常用。

#### 例子
在二元分类问题中，如图像分类、垃圾邮件检测等，sigmoid函数能够输出一个介于0和1之间的值，代表预测为类别1的概率。

**正确答案选项：C**

---

## 第 26 题

**假设你已经建立了一个神经网络。您决定将权重和偏差初始化为零。以下哪项陈述是正确的？（选出所有正确项）**

**A.** 第一隐藏层中的每个神经元将执行相同的计算。因此，即使在梯度下降的多次迭代之后，层中的每个神经元将执行与其他神经元相同的计算。

**B.** 第一隐层中的每个神经元在第一次迭代中执行相同的计算。但是在梯度下降的一次迭代之后，他们将学会计算不同的东西，因为我们已经“破坏了对称性”。

**C.** 第一个隐藏层中的每个神经元将执行相同的计算，但不同层中的神经元执行不同的计算，因此我们完成了课堂上所描述的“对称性破坏”。

**D.** 即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算，因此，它们的参数会以自己的方式不断演化。

#### 详解
如果将权重初始化为零，那么所有神经元将执行相同的计算，并且会计算出相同的梯度。这种现象称为“对称性问题”，它会导致所有神经元在训练中无法学习到不同的特征。因此，A 是正确的描述。选项 B、C、D 都是错误的，因为对称性没有被破坏，所有神经元都会在整个训练过程中保持相同的计算。

#### 例子
在训练神经网络时，如果初始化所有权重为零，隐藏层的神经元会学习到完全相同的内容，无法实现网络的多样性和学习能力。

**正确答案选项：A**

---

## 第 27 题

**逻辑回归的权重w应该随机初始化，而不是全部初始化为全部零，否则，逻辑回归将无法学习有用的决策边界，因为它将无法“打破对称”**

**A.** 对  
**B.** 不对

#### 详解
在逻辑回归中，虽然只有一个输出神经元，但随机初始化权重仍然是一个好习惯。将所有权重初始化为零将导致模型永远无法学到任何有效的信息，因为所有输入的影响将完全相同，无法破坏对称性。

#### 例子
在训练逻辑回归时，如果将权重初始化为零，梯度下降不会收敛到有意义的权重值。

**正确答案选项：A**

---

## 第 28 题

**你已经为所有隐藏的单位建立了一个使用tanh激活的网络。使用np.random.randn(…, …)*1000将权重初始化为相对较大的值。会发生什么？**

**A.** 没关系。只要随机初始化权重，梯度下降不受权重大小的影响。  
**B.** 这将导致tanh的输入也非常大，从而导致梯度也变大。因此，你必须将$\alpha$设置得非常小，以防止发散；这将减慢学习速度。  
**C.** 这将导致tanh的输入也非常大，导致单元被“高度激活”。与权重从小值开始相比，加快了学习速度。  
**D.** 这将导致tanh的输入也非常大，从而导致梯度接近于零。因此，优化算法将变得缓慢。

#### 详解
如果权重初始化过大，tanh函数的输入值会非常大，导致其输出接近-1或1。在这种情况下，tanh函数的梯度几乎为零，这会导致梯度消失问题，导致优化算法变得非常缓慢。

#### 例子
在深度神经网络中，如果权重初始化过大，激活函数会进入“饱和区”，无法有效地传播梯度。

**正确答案选项：D**

---

## 第 30 题

**在和上一问相同的网络中，$Z^{[1]}$ 和 $A^{[1]}$的维度是多少？**

**A.** $Z^{[1]}$ 和 $A^{[1]}$是(4, 1)  
**B.** $Z^{[1]}$ 和 $A^{[1]}$是(1, 4)  
**C.** $Z^{[1]}$ 和 $A^{[1]}$是(4, m)  
**D.** $Z^{[1]}$ 和 $A^{[1]}$是(4, 2)

#### 详解
通常情况下，$Z^{[1]}$和$A^{[1]}$的维度取决于当前层的神经元数量和输入样本数量。如果第1层有4个神经元，且有$m$个训练样本，那么$Z^{[1]}$和$A^{[1]}$的维度应为(4, m)。

#### 例子
对于一个含有5个神经元和3个训练样本的隐藏层，$Z$和$A$的维度将为(5, 3)。

**正确答案选项：C**

----























