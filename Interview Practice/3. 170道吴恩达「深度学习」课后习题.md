# 170道吴恩达「深度学习」课后习题 第三部分

## 第 101 题

**你认为把下面这个过滤器应用到灰度图像会怎么样？**

<img width="987" alt="Screen Shot 2024-09-28 at 10 39 57 PM" src="https://github.com/user-attachments/assets/06a2782b-a8e0-4f2f-a87a-ac0d8210c499">

**A.** 会检测45度边缘

**B.** 会检测垂直边缘

**C.** 会检测水平边缘

**D.** 会检测图像对比度

---

## 第 102 题

**假设你的输入是一个300×300的彩色（RGB）图像，而你没有使用卷积神经网络。如果第一个隐藏层有100个神经元，每个神经元与输入层进行全连接，那么这个隐藏层有多少个参数（包括偏置参数）？**

**A.** 9,000,001  
**B.** 9,000,100  
**C.** 27,000,001  
**D.** 27,000,100

#### 详解
输入图像有300×300×3 = 270,000个输入值。每个神经元有270,000个权重，再加上一个偏置参数，因此每个神经元有270,001个参数。100个神经元总共有270,001 × 100 = 27,000,100个参数。

**正确答案选项：D**

---

## 第 103 题

**假设你的输入是300×300彩色（RGB）图像，并且你使用卷积层和100个过滤器，每个过滤器都是5×5的大小，请问这个隐藏层有多少个参数（包括偏置参数）？**

**A.** 2501  
**B.** 2600  
**C.** 7500  
**D.** 7600

#### 详解
每个卷积过滤器的大小是5×5×3（因为是RGB图像）= 75，加上一个偏置参数，因此每个过滤器有76个参数。100个过滤器共有76 × 100 = 7600个参数。

**正确答案选项：D**

---

## 第 104 题

**你有一个63x63x16的输入，并使用大小为7x7的32个过滤器进行卷积，使用步幅为2和无填充，请问输出是多少？**

**A.** 29x29x32  
**B.** 16x16x32  
**C.** 29x29x16  
**D.** 16x16x16

#### 详解
输出尺寸的公式是：  
\[ \frac{(n - f)}{s} + 1 \]  
其中n是输入大小，f是过滤器大小，s是步幅。  
在本例中：  
\[ \frac{(63 - 7)}{2} + 1 = 29 \]  
因此输出为29x29，并且有32个过滤器，输出深度为32。

**正确答案选项：A**

---

## 第 105 题

**你有一个15x15x8的输入，并使用“pad = 2”进行填充，填充后的尺寸是多少？**

**A.** 17x17x10  
**B.** 19x19x8  
**C.** 19x19x12  
**D.** 17x17x8

#### 详解
填充后，新的尺寸为 \( n_H + 2 \times pad \) 和 \( n_W + 2 \times pad \)，即15 + 2×2 = 19。深度不受填充影响，仍然为8。

**正确答案选项：B**

---

## 第 106 题

**你有一个63x63x16的输入，有32个过滤器进行卷积，每个过滤器的大小为7x7，步幅为1，你想要使用“same”的卷积方式，请问pad的值是多少？**

**A.** 1  
**B.** 2  
**C.** 3  
**D.** 7

#### 详解
对于“same”卷积，pad的值为 \( pad = \frac{f - 1}{2} \)，即 \( \frac{7 - 1}{2} = 3 \)。

**正确答案选项：C**

---

## 第 107 题

**你有一个32x32x16的输入，并使用步幅为2、过滤器大小为2的最大化池，请问输出是多少？**

**A.** 15x15x16  
**B.** 16x16x8  
**C.** 16x16x16  
**D.** 32x32x8

#### 详解
池化层的输出尺寸计算为：
\[ \frac{(n - f)}{s} + 1 \]  
即 \( \frac{(32 - 2)}{2} + 1 = 16 \)，因此输出为16x16x16。

**正确答案选项：C**

---

## 第 108 题

**因为池化层不具有参数，所以它们不影响反向传播的计算。**

**A.** 对  
**B.** 不对

#### 详解
虽然池化层没有参数，但它仍然影响反向传播中的梯度流，因为它影响了前向传播中激活值的传播。

**正确答案选项：B**

---

## 第 109 题

**关于“参数共享”是使用卷积网络的好处。关于参数共享的下列哪个陈述是正确的？（选出所有正确项）**

**A.** 它减少了参数的总数，从而减少过拟合。  
**B.** 它允许在整个输入值的多个位置使用特征检测器。  
**C.** 它允许为一项任务学习的参数即使对于不同的任务也可以共享（迁移学习）。  
**D.** 它允许梯度下降将许多参数设置为零，从而使得连接稀疏。

#### 详解
参数共享减少了参数总数，并且允许特征检测器在输入的不同位置重复使用。

**正确答案选项：A, B**

---

## 第 110 题

**在课堂上，我们讨论了“稀疏连接”是使用卷积层的好处。这是什么意思?**

**A.** 正则化导致梯度下降将许多参数设置为零。  
**B.** 每个过滤器都连接到上一层的每个通道。  
**C.** 下一层中的每个激活只依赖于前一层的少量激活。  
**D.** 卷积网络中的每一层只连接到另外两层。

#### 详解
“稀疏连接”是指每个卷积核只与输入层的局部区域连接，而不是与所有神经元连接。

**正确答案选项：C**

---

## 第 111 题

**在典型的卷积神经网络中，随着网络的深度增加，你能看到的现象是？**

**A.** \(n_H\) 和 \(n_W\) 增加，同时 \(n_C\) 减少  
**B.** \(n_H\) 和 \(n_W\) 减少，同时 \(n_C\) 也减少  
**C.** \(n_H\) 和 \(n_W\) 增加，同时 \(n_C\) 也增加  
**D.** \(n_H\) 和 \(n_W\) 减少，同时 \(n_C\) 增加

#### 详解
在卷积神经网络中，随着网络深度的增加，特征图的高度和宽度 (\(n_H\) 和 \(n_W\)) 通常会减小，而特征图的深度 (\(n_C\)) 会增加。这是因为卷积和池化操作逐步减少空间维度，而网络提取的特征数量会增加。

**正确答案选项：D**

---

## 第 112 题

**在典型的卷积神经网络中，你能看到的是？**

**A.** 多个卷积层后面跟着的是一个池化层  
**B.** 多个池化层后面跟着的是一个卷积层  
**C.** 全连接层（FC）位于最后的几层  
**D.** 全连接层（FC）位于开始的几层

#### 详解
典型的卷积神经网络由多个卷积层和池化层的交替组成，最终通过几个全连接层来进行分类任务。全连接层通常位于网络的最后部分。

**正确答案选项：A, C**

---

## 第 113 题

**为了构建一个非常深的网络，我们经常在卷积层使用“valid”的填充，只使用池化层来缩小激活值的宽/高度，否则的话就会使得输入迅速的变小。**

**A.** 对  
**B.** 不对

#### 详解
如果使用“valid”填充，卷积层会使输入尺寸变小，这将导致输入很快缩小到非常小的尺寸。为了避免这个问题，通常会在卷积层中使用“same”填充，以保持输入尺寸不变，只有池化层用于减小尺寸。

**正确答案选项：B**

---

## 第 114 题

**训练更深层的网络（例如，在网络中添加额外的层）可以使网络适应更复杂的功能，因此几乎总是会导致更低的训练错误。对于这个问题，假设是指“普通”网络**

**A.** 对  
**B.** 不对

#### 详解
随着网络深度增加，理论上网络能够拟合更复杂的函数。然而，实际中更深的网络容易出现训练困难或梯度消失/爆炸等问题，这可能导致更高的训练误差。因此，简单增加网络深度不总是会降低训练错误。

**正确答案选项：B**

---

## 第 115 题

**下面计算残差(ResNet)块的公式中，横线上应该分别填什么？**

<img width="987" alt="Screen Shot 2024-09-28 at 10 41 48 PM" src="https://github.com/user-attachments/assets/d49cd141-fa0d-41f0-a2af-8f48cd4c1022">

**A.** 分别是 $0$ 与 $z^{[l+1]}$

**B.** 分别是 $a^{[l]}$ 与 $0$

**C.** 分别是 $z^{[l]}$ 与 $a^{[l]}$

**D.** 分别是 $0$ 与 $a^{[l]}$


---

## 第 116 题

**关于残差网络下面哪个（些）说法是正确的？**

**A.** 使用跳越连接能够对反向传播的梯度下降有益，且能够帮你对更深的网络进行训练  
**B.** 跳跃连接计算输入的复杂的非线性函数以传递到网络中的更深层  
**C.** 有L层的残差网络一共有 \(L^2\) 种跳跃连接的顺序  
**D.** 跳跃连接能够使得网络轻松地学习残差块类的输入输出间的身份映射

#### 详解
残差网络使用跳跃连接，这有助于解决梯度消失的问题，帮助更深层的网络进行训练，并且它还允许网络轻松地学习输入输出间的恒等映射（identity mapping）。

**正确答案选项：A, D**

---

## 第 117 题

**假设你的输入的维度为64x64x16，单个1x1的卷积过滤器含有多少个参数（包括偏差）？**

**A.** 2  
**B.** 17  
**C.** 4097  
**D.** 1

#### 详解
1x1卷积过滤器在每个通道上有一个权重，总共有16个通道，因此共有16个权重参数，加上一个偏置参数，总共有17个参数。

**正确答案选项：B**

---

## 第 118 题

**假设你有一个维度为 \(n_H×n_W×n_C\) 的卷积输入，下面哪个说法是正确的（假设卷积层为1x1，步长为1，padding为0）？**

**A.** 你能够使用1x1的卷积层来减少 \(n_C\)，但是不能减少 \(n_H,n_W\)  
**B.** 你可以使用池化层减少 \(n_H,n_W\)，但是不能减少 \(n_C\)  
**C.** 你可以使用一个1x1的卷积层来减少 \(n_H,n_W\) 和 \(n_C\)  
**D.** 你可以使用池化层减少 \(n_H,n_W\) 和 \(n_C\)

#### 详解
1x1卷积层可以改变通道数 \(n_C\) 而不影响 \(n_H\) 和 \(n_W\)。而池化层可以减小 \(n_H\) 和 \(n_W\)，但不能改变 \(n_C\)。

**正确答案选项：A, B**

---

## 第 119 题

**关于 Inception 网络下面哪些说法是正确的**

**A.** Inception 网络包含了各种网络的体系结构（类似于随机删除节点模式，它会在每一步中随机选择网络的结构），因此它具有随机删除节点的正则化效应。  
**B.** Inception 块通常使用1x1的卷积来减少输入卷积的大小，然后再使用3x3和5x5的卷积。  
**C.** 一个inception 块允许网络使用1x1, 3x3, 5x5 的和卷积个池化层的组合。  
**D.** 通过叠加inception块的方式让inception网络更深，不会损害训练集的表现。

#### 详解
Inception网络使用1x1卷积来减少输入深度，接着使用不同尺寸的卷积核（1x1、3x3、5x5）和池化操作。通过组合这些操作，它可以学习不同尺度的特征，同时增加网络深度而不损害训练表现。

**正确答案选项：B, C, D**

---

## 第 120 题

**下面哪些是使用卷积网络的开源实现（包含模型/权值）的常见原因？**

**A.** 为一个计算机视觉任务训练的模型通常可以用来数据扩充，即使对于不同的计算机视觉任务也是如此。  
**B.** 为一个计算机视觉任务训练的参数通常对其他计算机视觉任务的预训练是有用的。  
**C.** 使用获得计算机视觉竞赛奖项的相同的技术，广泛应用于实际部署。  
**D.** 使用开源实现可以很简单的来实现复杂的卷积结构。

#### 详解
预训练的模型参数通常可以用于其他任务，节省训练时间，并且开源实现可以简化复杂卷积结构的构建过程，帮助你使用最新的技术应用于实际问题。

**正确答案选项：B, C, D**

---

## 第 121 题

**现在你要构建一个能够识别三个对象并定位位置的算法，这些对象分别是：行人（c=1），汽车（c=2），摩托车（c=3）。下图中的标签哪个是正确的？** 注：$y=[p_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3]$

![image](https://github.com/user-attachments/assets/9678c628-f657-4633-b58e-7f8545ac1f7b)

**A.** y=[1, 0.3, 0.7, 0.3, 0.3, 0, 1, 0]

**B.** y=[1, 0.7, 0.5, 0.3, 0.3, 0, 1, 0]

**C.** y=[1, 0.3, 0.7, 0.5, 0.5, 0, 1, 0]

**D.** y=[1, 0.3, 0.7, 0.5, 0.5, 1, 0, 0]

**E.** y=[0, 0.2, 0.4, 0.5, 0.5, 0, 1, 0]



---

## 第 122 题

**继续上一个问题，下图中y的值是多少？注：“？”是指“不关心这个值”，这意味着神经网络的损失函数不会关心神经网络对输出的结果，和上面一样，** $y=[p_c,b_x,b_y,b_h,b_w,c_1,c_2,c_3]$

![image](https://github.com/user-attachments/assets/aaba02f5-6f14-4054-b859-fdce00faeb62)

**A.** y=[1, ?, ?, ?, ?, 0, 0, 0]

**B.** y=[0, ?, ?, ?, ?, ?, ?, ?]

**C.** y=[?, ?, ?, ?, ?, ?, ?, ?]

**D.** y=[0, ?, ?, ?, ?, 0, 0, 0]

**E.** y=[1, ?, ?, ?, ?, ?, ?, ?]



---

## 第 123 题

**你现在任职于自动化工厂中，您的系统将看到一罐饮料沿着传送带向下移动，你要对其进行拍照，然后确定照片中是否有饮料罐，如果有的话就对其进行包装。饮料罐头是圆的，而包装盒是方的，每一罐饮料的大小是一样的，每个图像中最多只有一罐饮料，现在你有下面的方案可供选择，这里有一些训练集图像：**

![image](https://github.com/user-attachments/assets/422e827f-e333-486b-b4b6-d68f7c330e7e)

**你的神经网络最合适的输出单位是什么？**

**A.** 逻辑单元(用于分类图像中是否有罐头)
**B.** 逻辑单元，$b_x$ 和 $b_y$
**C.** 逻辑单元，$b_x,b_y,b_h$ (因为$b_w,b_h$，所以只需要一个就行了)
**D.** 逻辑单元，$b_x,b_y,b_h,b_w$



---

## 第 124 题

**如果你想要构建一个能够输入人脸图片，输出为N个标记的神经网络（假设图像只包含一张脸），那么你的神经网络有多少个输出节点？**

**A.** N  
**B.** 2N  
**C.** 3N  
**D.** \(N^2\)

#### 详解
每个标记需要一个输出节点来预测该标记的概率，因此输出节点的数量应与标记数N相同。

**正确答案选项：A**

---

## 第 125 题

**在训练课程中描述的一个对象检测系统中，您需要一个训练集，其中包含许多要检测的对象的图片。但是，由于该算法可以学习自检测对象，因此不需要在训练集中提供边界框。**

**A.** 正确  
**B.** 错误

#### 详解
在对象检测中，训练集中需要包含对象的边界框信息，否则算法无法学习对象的位置。

**正确答案选项：B**

---

## 第 126 题

**如你正在应用一个滑动窗口分类器（非卷积实现），增加步长不仅会提高准确性，也会降低成本。**

**A.** 正确  
**B.** 错误

#### 详解
增加步长会降低计算成本，因为需要进行的滑动窗口操作变少，但准确性可能会下降，因为较大的步长可能会错过一些重要细节。

**正确答案选项：B**

---

## 第 127 题

**在YOLO算法中，在训练时，只有一个单元（该单元包含对象的中心/中点）负责检测这个对象**

**A.** 正确  
**B.** 错误

#### 详解
在YOLO算法中，只有包含对象中心的那个单元负责预测该对象。

**正确答案选项：A**

---

## 第 128 题

**这两个框中 IoU 大小是多少？左上角的框是2x2大小，右下角的框是2x3大小，重叠部分是1x1**

![image](https://github.com/user-attachments/assets/a135758a-8f9d-4d1f-b0f0-ee29fe3d9b7d)

**A.** 1/6
**B.** 1/9
**C.** 1/10
**D.** 以上都不是

---

## 第 129 题

**假如你在下图中的预测框中使用非最大值抑制，其参数是放弃概率≤ 0.4的框，并决定两个框IoU的阈值为0.5，使用非最大值抑制后会保留多少个预测框？**

![image](https://github.com/user-attachments/assets/cd794da0-3ef7-492f-ba74-bb7da1afed36)

**A.** 3
**B.** 4
**C.** 5
**D.** 6
**E.** 7



---

## 第 130 题

**假如你使用YOLO算法，使用19x19格子来检测20个分类，使用5个锚框（anchor box）。在训练的过程中，对于每个图像你需要输出卷积后的结果y作为神经网络目标值（这是最后一层），$y$可能包括一些“？”或者“不关心的值”。请问最后的输出维度是多少？**

**A.** 19x19x(25x20)  
**B.** 19x19x(20x25)  
**C.** 19x19x(5x25)  
**D.** 19x19x(5x20)

#### 详解
对于每个锚框，需要预测20个类别概率、4个边界框坐标和1个置信度，总共25个输出。由于有5个锚框，因此输出维度为19x19x(5x25)。

**正确答案选项：C**

---

## 第 131 题

**面部验证只需要将新图片与1个人的面部进行比较，而面部识别则需要将新图片与K个人的面部进行比较。**

**A.** 正确  
**B.** 错误

#### 详解
面部验证是将新图片与一个已知面部进行比较，而面部识别需要与多个已知面部进行比较。

**正确答案选项：A**

---

## 第 132 题

**在人脸验证中函数d(img1,img2)起什么作用？**

**A.** 只需要给出一个人的图片就可以让网络认识这个人  
**B.** 为了解决一次学习的问题  
**C.** 这可以让我们使用softmax函数来学习预测一个人的身份，在这个单元中分类的数量等于数据库中的人的数量加1  
**D.** 鉴于我们拥有的照片很少，我们需要将它运用到迁移学习中

#### 详解
函数 \(d(img1, img2)\) 是用于计算两张图片之间的距离或相似度，以实现一次学习的目的。

**正确答案选项：B**

---

## 第 133 题

**为了训练人脸识别系统的参数，使用包含了10万个不同的人的10万张图片的数据集进行训练是合理的。**

**A.** 正确  
**B.** 错误

#### 详解
由于每个人仅有一张图片，这样的数据集难以学习到每个人的特征，因此不是很合理。

**正确答案选项：B**

---

## 第 134 题

**下面哪个是三元组损失的正确定义（请把\(\alpha\) 也考虑进去）？**

**A.** \(max(\left \|f(A)−f(P)\right \|^2−\left \|f(A)−f(N)\right \|^2+\alpha,0)\)  
**B.** \(max(\left \|f(A)−f(N)\right \|^2−\left \|f(A)−f(P)\right \|^2+\alpha,0)\)  
**C.** \(max(\left \|f(A)−f(N)\right \|^2−\left \|f(A)−f(P)\right \|^2−\alpha,0)\)  
**D.** \(max(\left \|f(A)−f(P)\right \|^2−\left \|f(A)−f(N)\right \|^2−\alpha,0)\)

#### 详解
三元组损失函数的目的是使 \(f(A)\) 和 \(f(P)\) 更接近，并使 \(f(A)\) 和 \(f(N)\) 更远。因此应选择A选项。

**正确答案选项：A**

---

## 第 135 题

**在下图中的孪生卷积网络(Siamese network)结构图中**

![image](https://github.com/user-attachments/assets/cc800c24-7915-40dd-accd-8c683c9ce699)

**上下两个神经网络拥有不同的输入图像，但是其中的网络参数是完全相同的**

**A.** 正确
**B.** 错误


----

## 第 136 题

**你在一个拥有100种不同的分类的数据集上训练一个卷积神经网络，你想要知道是否能够找到一个对猫的图片很敏感的隐藏节点（即在能够强烈激活该节点的图像大多数都是猫的图片的节点），你更有可能在第4层找到该节点而不是在第1层更有可能找到。**

**A.** 正确  
**B.** 错误

#### 详解
较深的网络层学到的是更高级的特征，因此第4层更有可能对猫等高层特征敏感。

**正确答案选项：A**

---

## 第 137 题

**神经风格转换被训练为有监督的学习任务，其中的目标是输入两个图像 (x)，并训练一个能够输出一个新的合成图像(y)的网络**

**A.** 正确  
**B.** 错误

#### 详解
神经风格转换是非监督任务，因为它不需要目标输出，而是基于损失函数优化图像。

**正确答案选项：B**

---

## 第 138 题

**在一个卷积网络的深层，每个通道对应一个不同的特征检测器，风格矩阵 \(G^{[l]}\) 度量了l层中不同的特征探测器的激活（或相关）程度**

**A.** 正确  
**B.** 错误

#### 详解
风格矩阵 \(G^{[l]}\) 是特征图的“Gram矩阵”，确实度量了特征探测器间的相关性。

**正确答案选项：A**

---

## 第 139 题

**在神经风格转换中，在优化算法的每次迭代中更新的是什么？**

**A.** 神经网络的参数  
**B.** 生成图像G的像素值  
**C.** 正则化参数  
**D.** 内容图像C的像素值

#### 详解
神经风格转换的优化目标是生成图像G的像素值。

**正确答案选项：B**

---

## 第 140 题

**你现在用拥有的是3D的数据，现在构建一个网络层，其输入的卷积是32×32×32×16（此卷积有16个通道），对其使用32个3×3×3的过滤器（无填充，步长为1）进行卷积操作，请问输出的卷积是多少？**

**A.** 30×30×30×32  
**B.** 不能操作，因为指定的维度不匹配，所以这个卷积步骤是不可能执行的  
**C.** 30×30×30×16

#### 详解
输出维度的计算：无填充，步长为1，大小减少到 (32 - 3 + 1) = 30，通道数为过滤器数量32。

**正确答案选项：A**

---

## 第 141 题

**假设你的训练样本是句子(单词序列)，下面哪个选项指的是第i个训练样本中的第j个词?**

**A.** $x^{(i)}$

**B.** $x^{(j)}$

**C.** $x^{(j)}$

**D.** $x^{(i)}$



---

## 第 142 题

**看一下下面的这个循环神经网络：**

![image](https://github.com/user-attachments/assets/df48f3db-7328-4ee6-a919-b13efe9e55d1)

**在下面的条件中，满足上图中的网络结构的参数是：**

**A.** $T_x=T_y$
**B.** $T_x 
**C.** $T_x>T_y$
**D.** $T_x=1$



----

## 第 143 题

**这些任务中的哪一个会使用多对一的RNN体系结构？**

![image](https://github.com/user-attachments/assets/08dfddd0-f69e-4e8e-ae05-14ec8ae4774c)

**A.** 语音识别（输入语音，输出文本）

**B.** 情感分类（输入一段文字，输出0或1表示正面或者负面的情绪）

**C.** 图像分类（输入一张图片，输出对应的标签）

**D.** 人声性别识别（输入语音，输出说话人的性别）




----

## 第 144 题

**假设你现在正在训练下面这个RNN的语言模型：**

![image](https://github.com/user-attachments/assets/be0eeb89-02c8-44aa-9a90-c3b0b210362a)

**在$t$时，这个RNN在做什么？**

**A.** 计算$P(y^{<1>},y^{<2>},…,y^{})$

**B.** 计算$P(y^{})$

**C.** 计算$P(y^{}∣y^{<1>},y^{<2>},…,y^{})$

**D.** 计算$P(y^{}∣y^{<1>},y^{<2>},…,y^{})$




----

## 第 145 题

**你已经完成了一个语言模型RNN的训练，并用它来对句子进行随机取样，如下图：**

![image](https://github.com/user-attachments/assets/5ab05c57-b82c-43f4-9f5c-5b3d7c780496)

**在每个时间步tt都在做什么？**

**A.** (1)使用RNN输出的概率，选择该时间步的最高概率单词作为$\hat{y}^{}$，(2)然后将训练集中的正确的单词传递到下一个时间步

**B.** (1)使用由RNN输出的概率将该时间步的所选单词进行随机采样作为$\hat{y}^{}$，(2)然后将训练集中的实际单词传递到下一个时间步

**C.** (1)使用由RNN输出的概率来选择该时间步的最高概率词作为$\hat{y}^{}$，(2)然后将该选择的词传递给下一个时间步

**D.** (1)使用RNN该时间步输出的概率对单词随机抽样的结果作为$\hat{y}^{}$，(2)然后将此选定单词传递给下一个时间步




----

## 第 146 题

**你正在训练一个RNN网络，你发现你的权重与激活值都是“NaN”，下列选项中，哪一个是导致这个问题的最有可能的原因？**

**A.** 梯度消失  
**B.** 梯度爆炸  
**C.** ReLU函数作为激活函数g(.)，在计算g(z)时，z的数值过大了  
**D.** Sigmoid函数作为激活函数g(.)，在计算g(z)时，z的数值过大了

#### 详解
梯度爆炸会导致数值过大，最终导致NaN。

**正确答案选项：B**

---

## 第 147 题

**假设你正在训练一个LSTM网络，你有一个10,000词的词汇表，并且使用一个激活值维度为100的LSTM块，在每一个时间步中，\(\Gamma_u\) 的维度是多少？**

**A.** 1  
**B.** 100  
**C.** 300  
**D.** 10,000

#### 详解
\(\Gamma_u\) 表示更新门，是LSTM内部维度，应该与激活值维度一致。

**正确答案选项：B**

---

## 第 148 题

**这里有一些GRU的更新方程：**

![image](https://github.com/user-attachments/assets/937f63cd-ebe4-4ddb-ae5a-6e58a59fb8aa)

**爱丽丝建议通过移除 $\Gamma _u$来简化GRU，即设置$\Gamma _u=1$。贝蒂提出通过移除$\Gamma _R$来简化GRU，即设置$\Gamma _R=1$。哪种模型更容易在梯度不消失问题的情况下训练，即使在很长的输入序列上也可以进行训练？**

**A.** 爱丽丝的模型（即移除$\Gamma _u$），因为对于一个时间步而言，如果$\Gamma _r \approx 0$，梯度可以通过时间步反向传播而不会衰减。

**B.** 爱丽丝的模型（即移除$\Gamma _u$），因为对于一个时间步而言，如果$\Gamma _r \approx 1$，梯度可以通过时间步反向传播而不会衰减。

**C.** 贝蒂的模型（即移除$\Gamma _r$），因为对于一个时间步而言，如果$\Gamma _u \approx 0$，梯度可以通过时间步反向传播而不会衰减。

**D.** 贝蒂的模型（即移除$\Gamma _r$），因为对于一个时间步而言，如果$\Gamma _u \approx 1$，梯度可以通过时间步反向传播而不会衰减。




---

## 第 149 题

**这里有一些GRU和LSTM的方程：**

![image](https://github.com/user-attachments/assets/a74ca033-da0e-4a02-bea7-7d0875a0b36f)

**从这些我们可以看到，在LSTM中的更新门和遗忘门在GRU中扮演类似___与___的角色，空白处应该填什么？**

**A.** $\Gamma _u$与$1-\Gamma _u$

**B.** $\Gamma _u$与$\Gamma _r$

**C.** $1-\Gamma _u$与$\Gamma _u$

**D.** $\Gamma _r$与$\Gamma _u$




---

## 第 150 题

**你有一只宠物狗，它的心情很大程度上取决于当前和过去几天的天气。你已经收集了过去365天的天气数据\(x^{<1>},…,x^{<365>}\)，这些数据是一个序列，你还收集了你的狗心情的数据\(y^{<1>},…,y^{<365>}\)，你想建立一个模型来从x到y进行映射，你应该使用单向RNN还是双向RNN来解决这个问题？**

**A.** 双向RNN，因为在\(t\)日的情绪预测中可以考虑到更多的信息。  
**B.** 双向RNN，因为这允许反向传播计算中有更精确的梯度。  
**C.** 单向RNN，因为\(y^{}\)的值仅依赖于\(x^{<1>},…,x^{}\)，而不依赖于\(x^{},…,x^{<365>}\)  
**D.** 单向RNN，因为\(y^{}\)的值只取决于\(x^{}\)，而不是其他天的天气。

#### 详解
宠物狗的情绪应只与过去和当前的天气相关，不需要未来的信息，因此单向RNN是更好的选择。

**正确答案选项：C**

----

## 第 151 题

**假设你为10000个单词学习词嵌入，为了捕获全部范围的单词的变化以及意义，那么词嵌入向量应该是10000维的。**

**A.** 正确  
**B.** 错误

#### 详解
词嵌入向量的维度通常远小于词汇表的大小（比如100-300维），因为词汇的变化和意义可以被映射到一个较低维度的向量空间中。

**正确答案选项：B**

---

## 第 152 题

**什么是t-SNE？**

**A.** 一种非线性降维算法  
**B.** 一种能够解决词向量上的类比的线性变换  
**C.** 一种用于学习词嵌入的监督学习算法  
**D.** 一个开源序列模型库

#### 详解
t-SNE（t-distributed Stochastic Neighbor Embedding）是一种用于可视化高维数据的非线性降维算法。

**正确答案选项：A**

---

## 第 153 题

**假设你下载了一个已经在一个很大的文本语料库上训练过的词嵌入的数据，然后你要用这个词嵌入来训练RNN并用于识别一段文字中的情感，判断这段文字的内容是否表达了“快乐”。**

<img width="987" alt="Screen Shot 2024-09-28 at 10 55 39 PM" src="https://github.com/user-attachments/assets/08c0ae0a-5d2e-41e5-bca9-e010054e2935">

**那么即使 “欣喜若狂” 这个词没有出现在你的小训练集中，你的RNN也会认为“我欣喜若狂”应该被贴上 $y = 1$ 的标签。**

**A.** 正确
**B.** 错误





---

## 第 154 题

**对于词嵌入而言，下面哪一个（些）方程是成立的？**

**A.** \(e_{boy}−e_{girl} \approx e_{brother}−e_{sister}\)  
**B.** \(e_{boy}−e_{girl} \approx e_{sister}−e_{brother}\)  
**C.** \(e_{boy} - e_{brother} \approx e_{girl} - e_{sister}\)  
**D.** \(e_{boy} - e_{brother} \approx e_{sister} - e_{girl}\)

#### 详解
词嵌入向量之间的关系应当保持语义上的一致性，例如 "boy" 对应 "girl" 的关系应当与 "brother" 对应 "sister" 的关系相似。

**正确答案选项：A**

---

## 第 155 题

**设 \(E\) 为嵌入矩阵，\(e_{1234}\) 对应的是词“1234”的独热向量，为了获得1234的词嵌入，为什么不直接在Python中使用代码 \(E \ast e_{1234}\) 呢？**

**A.** 因为这个操作是在浪费计算资源  
**B.** 因为正确的计算方式是 \(E^T ∗ e_{1234}\)  
**C.** 因为它没有办法处理未知的单词（<UNK>）  
**D.** 以上全都不对，因为直接调用 \(E ∗ e_{1234}\) 是最好的方案

#### 详解
独热向量只有一个位置是1，其余全为0，直接调用 \(E \ast e_{1234}\) 会浪费计算资源，而 \(E[:, 1234]\) 可以直接返回结果。

**正确答案选项：A**

---

## 第 156 题

**在学习词嵌入时，我们创建了一个预测 \(P(target \mid context)\) 的任务，如果这个预测做的不是很好那也是没有关系的，因为这个任务更重要的是学习了一组有用的嵌入词。**

**A.** 正确  
**B.** 错误

#### 详解
学习词嵌入的主要目的是获得能够捕捉语义信息的嵌入向量，而不是准确预测目标词。

**正确答案选项：A**

---

## 第 157 题

**在word2vec算法中，你要预测 \(P(t \mid c)\)，其中 \(t\) 是目标词（target word），\(c\) 是语境词（context word）。你应当在训练集中怎样选择 \(t\) 与 \(c\) 呢？**

**A.** \(c\) 与 \(t\) 应当在附近词中  
**B.** \(c\) 是在 \(t\) 前面的一个词  
**C.** \(c\) 是 \(t\) 之前句子中所有单词的序列  
**D.** \(c\) 是 \(t\) 之前句子中几个单词的序列

#### 详解
在word2vec中，目标是预测目标词 \(t\) 与其附近的上下文词 \(c\) 之间的关系。

**正确答案选项：A**

---

## 第 158 题

**假设你有1000个单词词汇，并且正在学习500维的词嵌入，word2vec模型使用下面的softmax函数：**

<img width="987" alt="Screen Shot 2024-09-28 at 10 56 47 PM" src="https://github.com/user-attachments/assets/ddf5d414-4c11-478d-bdf6-9aa828af84bb">

**以下说法中哪一个（些）是正确的？**

**A.** $\theta_t$与$e_c$都是500维的向量

**B.** $\theta_t$与$e_c$ 都是10000维的向量

**C.** $\theta_t$与$e_c%都是通过Adam或梯度下降等优化算法进行训练的

**D.** 训练之后，$\theta_t$应该非常接近$e_c$，因为ttt和ccc是一个词





---


## 第 159 题

**假设你有10000个单词词汇，并且正在学习500维的词嵌入，GloVe模型最小化了这个目标:**

<img width="987" alt="Screen Shot 2024-09-28 at 10 57 18 PM" src="https://github.com/user-attachments/assets/c0bcf870-bd16-48d9-b5ca-0141065f51a3">

**以下说法中哪一个（些）是正确的？**

**A.** $\theta_i$与$e_j$应当初始化为0

**B.** $\theta_i$与$e_je$应当使用随机数进行初始化

**C.** $X_{ij}$是单词i在j中出现的次数

**D.** 加权函数$f(.)$必须满足$f(0)=0$




---

## 第 160 题

**你已经在文本数据集 \(m_1\) 上训练了词嵌入，现在准备将它用于一个语言任务中，对于这个任务，你有一个单独标记的数据集 \(m_2\)，请记住，使用词嵌入是一种迁移学习的形式。在以下那种情况中，词嵌入会有帮助？**

**A.** \(m_1 \gg m_2\)  
**B.** \(m_1 \ll m_2\)

#### 详解
如果 \(m_1\) 数据集比 \(m_2\) 数据集大很多，迁移学习将会有更多的语义信息帮助任务。

**正确答案选项：A**

---

## 第 161 题

**想一想使用如下的编码-解码模型来进行机器翻译：**

![image](https://github.com/user-attachments/assets/894057c3-e22b-4297-a77c-b192b38476a1)

**这个模型是“条件语言模型”,编码器部分(绿色显示)的意义是建模中输入句子x的概率**

**A.** 正确
**B.** 错误




--- 


## 第 162 题

**在集束搜索中，如果增加集束宽度 \(b\)，以下哪一项是正确的？**

**A.** 集束搜索将运行的更慢  
**B.** 集束搜索将使用更多的内存  
**C.** 集束搜索通常将找到更好地解决方案（比如：在最大化概率 \(P(y|x)\) 上做的更好）  
**D.** 集束搜索将在更少的步骤后收敛

#### 详解
增加集束宽度 \(b\) 会增加计算量和内存需求，同时可能找到更好的解。

**正确答案选项：A, B, C**

---

## 第 163 题

**在机器翻译中，如果我们在不使用句子归一化的情况下使用集束搜索，那么算法会输出过短的译文。**

**A.** 正确  
**B.** 错误

#### 详解
在没有句子归一化的情况下，较短句子往往会有更高的总概率，因此算法可能偏向输出过短的译文。

**正确答案选项：A**

---

## 第 164 题

**假设你正在构建一个能够让语音片段$x$转为译文$y$的基于RNN模型的语音识别系统，你的程序使用了集束搜索来试着找寻最大的$P(y|x)$的值yyy。在开发集样本中，给定一个输入音频，你的程序会输出译文$\hat{y}$= “I’m building an A Eye system in Silly con Valley.”，人工翻译为$y^*$ = “I’m building an AI system in Silicon Valley.”**

**在你的模型中,**

<img width="987" alt="Screen Shot 2024-09-28 at 10 59 39 PM" src="https://github.com/user-attachments/assets/af60a673-2b02-4719-964e-d63996cb2686">

**那么，你会增加集束宽度$B$来帮助修正这个样本吗？**

**A.** 不会，因为$P(y^* \mid x) \leq P(\hat{y} \mid x)$说明了问题在RNN，而不是搜索算法

**B.** 不会，因为$P(y^* \mid x) \leq P(\hat{y} \mid x)$说明了问题在搜索算法，而不是RNN

**C.** 会的，因为$P(y^* \mid x) \leq P(\hat{y} \mid x)$说明了问题在RNN，而不是搜索算法

**D.** 会的，因为$P(y^* \mid x) \leq P(\hat{y} \mid x)$说明了问题在搜索算法，而不是RNN





---

## 第 165 题

**接着使用第4题的样本，假设你花了几周的时间来研究你的算法，现在你发现，对于绝大多数让算法出错的例子而言，\(P(y^* \mid x) \leq P(\hat{y} \mid x)\)，这表明你应该将注意力集中在改进搜索算法上，对吗？**

**A.** 正确  
**B.** 错误

#### 详解
如果 \(P(y^* \mid x) \leq P(\hat{y} \mid x)\)，表示搜索算法已经找到近似最佳结果，因此需要更多关注模型而非搜索算法。

**正确答案选项：B**

---

## 第 166 题

**回想一下机器翻译的模型：**


![image](https://github.com/user-attachments/assets/cfa549f3-ca75-4d8b-8d34-e413595911d6)


**除此之外，还有个公式$\alpha^{< t,t'>} = \frac{\text{exp}(e^{< t,t'>})}{\sum^{T_x}_{t'=1}\text{exp}(e^{< t,t'>})}$
下面关于$\alpha^{}$的选项那个（些）是正确的？**

**A.** 对于网络中与输出$y^{< t>}$高度相关的$\alpha^{< t'>}$而言，我们通常希望$\alpha^{}$的值更大（请注意上标）

**B.** 对于网络中与输出$y^{< t>}$高度相关的$\alpha^{< t>}$而言，我们通常希望$\alpha^{< t,t’>}$的值更大（请注意上标）

**C.** $\sum_t\alpha^{< t,t'>} = 1$(注意是和除以t)

**D.** $\sum_{t'}\alpha^{< t,t'>} = 1$(注意是和除以t')





---

## 第 167 题

**网络通过学习的值 \(e^{< t,t'>}\) 来学习在哪里关注“关注点”，这个值是用一个小的神经网络的计算出来的：这个神经网络的输入中，我们不能将 \(s^{< t>}\) 替换为 \(s^{< t-1>}\) 这是因为 \(s^{< t>}\) 依赖于 \(\alpha^{< t,t'>}\)，而 \(\alpha^{< t,t'>}\) 又依赖于 \(e^{< t,t'>}\)；所以在我们需要评估这个网络时，我们还没有计算出 \(s^{t}\)**

**A.** 正确  
**B.** 错误

#### 详解
注意力机制的权重计算不依赖当前的 \(s^{< t>}\)，而是依赖之前的 \(s^{< t-1>}\)。

**正确答案选项：A**

---

## 第 168 题

**与题1中的编码-解码模型（没有使用注意力机制）相比，我们希望有注意力机制的模型在下面的情况下有着最大的优势：**

**A.** 输入序列的长度 \(T_x\) 比较大  
**B.** 输入序列的长度 \(T_x\) 比较小

#### 详解
当输入序列长度较大时，注意力机制帮助模型更有效地关注相关信息。

**正确答案选项：A**

---

## 第 169 题

**在CTC模型下，不使用"空白"字符（_）分割的相同字符串将会被折叠。那么在CTC模型下，以下字符串将会被折叠成什么样子？__c_oo_o_kk___booooo\_oo__kkk**

**A.** cokbok  
**B.** cookbook  
**C.** cook book  
**D.** coookkboooooookkk

#### 详解
CTC模型将连续重复字符折叠为单个字符，忽略空白字符。

**正确答案选项：B**

---

## 第 170 题

**在触发词检测中，\(x^{< t>}\) 是：**

**A.** 时间 \(t\) 时的音频特征（就像是频谱特征一样）  
**B.** 第 \(t\) 个输入字，其被表示为一个独热向量或者一个字嵌入  
**C.** 是否在第 \(t\) 时刻说出了触发词  
**D.** 是否有人在第 \(t\) 时刻说完了触发词

#### 详解
在触发词检测任务中，\(x^{< t>}\) 通常表示时间 \(t\) 时的音频特征。

**正确答案选项：A**







