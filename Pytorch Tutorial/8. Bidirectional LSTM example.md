

```python
# Bidirectional LSTM

# Imports ( 7 imports and 8 lines )
import torch
import torchvision
# Todo: All neural network modules. nn.Linear, nn.Conv2d, BatchNorm, Loss functions
import torch.nn as nn
# Todo: For all Optimization algorithms, SGD, Adam, etc.
import torch.optim as optim
# Todo: All functions that don't have any parameters
import torch.nn.functional as F
# Todo: Gives easier dataset management and creates mini batches
from torch.utils.data import DataLoader
# Todo: Has standard datasets we can import in a nice way
import torchvision.datasets as datasets
# Todo: Transformations we can perform on our dataset
import torchvision.transforms as transforms

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
input_size = 28
sequence_length = 28
num_layers = 2

hidden_size = 256
num_classes = 10

learning_rate = 0.001
batch_size = 64
num_epochs = 2

# Create a bidirectional LSTM
class BRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):

        # Initialize the parent class
        super(BRNN, self).__init__()

        # Set hidden size and number of layers
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Define an LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)

        # Define a fully connected layer
        # hidden_size*2 because the LSTM is bidirectional, so it concatenates the outputs of two LSTMs
        # num_classes: Number of output classes
        self.fc = nn.Linear(hidden_size*2, num_classes)

    def forward(self, x):

        # Initialize hidden state and cell state with zeros
        # num_layers*2 because the LSTM is bidirectional
        h0 = torch.zeros(self.num_layers*2, x.size(0),self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers*2, x.size(0),self.hidden_size).to(device)

        # Forward propagate the LSTM
        # out: tensor of shape (batch_size, seq_length, hidden_size*2)
        out, _ = self.lstm(x, (h0, c0))

        # out is reshaped to collapse the sequence dimension, resulting in a 2D tensor of shape
        # out = out.reshape(out.shape[0], -1)
        # out = self.fc(out)

        # Use the output of the last time step for classification
        # out[:, -1, :] is the last time step's output for each sequence in the batch
        # 在前向传播过程中，LSTM 的输出 out 是一个三维张量，形状是 (batch_size, seq_length, hidden_size*2)。
        # batch_size 是每次输入的样本数量。
        # seq_length 是每个样本的时间步数（即序列长度）。
        # hidden_size*2 是每个时间步的特征维度，因为 LSTM 是双向的，所以有两倍的隐藏层大小。
        #
        # Todo: 为什么是要最后一时间步的信息？
        # 在使用双向 LSTM 时，模型会同时处理序列的正向和反向信息。
        # 对于每个时间步，正向和反向 LSTM 的输出会被合并
        # 最终得到的输出向量包含了序列的完整上下文。
        # 选择最后一个时间步的信息，就是选择了这一融合后的信息的最终表示。
        out = self.fc(out[:, -1, :])
        return out

# Load data
train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)

# Initialize network
model = BRNN(input_size, hidden_size, num_layers, num_classes).to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = learning_rate)

# Train network
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        # Get data to cuda if possible
        data = data.to(device=device).squeeze(1)
        target = target.to(device=device)

        # Get to correct shape
        # 训练数据时，你将数据重塑为 (batch_size, -1)
        # 但这是为全连接网络设计的，卷积神经网络不需要这个步骤。
        # data = data.reshape(data.shape[0],-1)

        # Forward
        output = model(data)
        loss = criterion(output, target)

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # Gradient descent or Adam step
        optimizer.step()

# Check accuracy on training & test to see how good our model

def check_accuracy(loader, model):

    if loader.dataset == train_dataset:
        print("Checking LSTM accuracy on training set")
    else:
        print("Checking LSTM accuracy on test set")

    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        for data, target in loader:

            # 这里很重要为什么要 .squeeze(1) ？？？
            # 因为 MNIST 数据集的图像原本是 (N, 1, 28, 28) 形状的 4D 张量
            # 而 LSTM 期望的输入形状是 (batch_size, sequence_length, input_size)
            # 即 3D 张量 (N, 28, 28)
            # 因此，需要移除维度为 1 的通道维度，以便将数据传递给 LSTM。
            # 具体来说，squeeze(1) 作用是移除第一个维度大小为 1 的维度。
            #
            # Todo: 示例说明
            #
            # 假设我们从 MNIST 数据集中加载一个批次的数据，其形状为 (64, 1, 28, 28)，其中：
            #
            # 64 是批次大小（batch size）
            # 1 是通道数（channel），因为 MNIST 是灰度图像
            # 28 是图像的高度（height）
            # 28 是图像的宽度（width）
            #
            # 但是，LSTM 期望输入形状为 (batch_size, sequence_length, input_size)
            # 对于 MNIST 数据集而言，这里：
            # batch_size 仍然是 64
            # sequence_length 是 28（高度）
            # input_size 是 28（宽度）
            # 因此，需要将形状从 (64, 1, 28, 28) 转换为 (64, 28, 28)，通过 squeeze(1) 移除大小为 1 的维度。
            # 通过这种方式，数据就变成了 LSTM 所期望的形状，可以传递给 LSTM 进行处理。

            data = data.to(device=device).squeeze(1)
            target = target.to(device=device)

            # Get to correct shape
            # 训练数据时，你将数据重塑为 (batch_size, -1)
            # 但这是为全连接网络设计的，卷积神经网络不需要这个步骤。
            # data = data.reshape(data.shape[0],-1)

            output = model(data)
            _,predictions = output.max(dim=1)

            num_correct += (predictions == target).sum()
            num_samples += predictions.size(0)

        print(f'Got {num_correct} / {num_samples} with LTSM accuracy {num_correct / num_samples * 100:.2f}')

    model.train()

check_accuracy(train_loader, model)
check_accuracy(test_loader, model)
```

