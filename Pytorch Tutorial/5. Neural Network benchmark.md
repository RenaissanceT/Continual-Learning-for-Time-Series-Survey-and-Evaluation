# Neural Network Benchmark 

## 第一步：搭建框架

```python
# Imports

# Create fully connected networks

# Set device

# Hyperparameters

# Load data

# Initialize network

# Loss and optimizer

# Train network

# Check accuracy on training & test to see how good our model

```


## 第二步：Imports

#### 1. 
```python
import torch
```
torch 是 PyTorch 的核心库，它提供了多种张量操作和自动求导功能。

#### 2.  
```python
import torch.nn as nn
```
`torch.nn` : 包含了构建神经网络的基本组件，例如各种层和损失函数。
- `nn.Module`: 所有神经网络模块的基类。
- `nn.Linear`: 全连接层。
- `nn.Conv2d`: 卷积层。
- `nn.ReLU`: 激活函数。

#### 3.  
```python
import torch.optim as optim
```

`torch.optim` 包含了各种优化算法，用于更新神经网络的参数以最小化损失函数。

- `optim.SGD`: 随机梯度下降优化器。
- `optim.Adam`: Adam 优化器。

#### 4.  
```python
import torch.nn.functional as F
```
`torch.nn.functional` 提供了函数式的接口来执行神经网络的各种操作和计算，例如激活函数、卷积操作等。
- `F.relu`: ReLU 激活函数。
- `F.softmax`: Softmax 函数。

#### 5.  
```python
from torch.utils.data import DataLoader
```

`DataLoader` 是 PyTorch 提供的数据加载工具，用于将数据集加载到内存中并批量化处理。

- `DataLoader`: 数据加载器，负责打乱数据、分批次加载。

#### 6.  
```python
import torchvision.datasets as datasets
```

`torchvision.datasets` 包含了许多常用的公共数据集，可以直接用于训练和测试模型。
- (Create a binibatch to test on)
- `datasets.MNIST`: MNIST 手写数字数据集。
- `datasets.CIFAR10`: CIFAR-10 图像数据集。

#### 7.  
```python
import torchvision.transforms as transforms
```
`torchvision.transforms` 提供了一些常用的数据预处理和增强操作。
- `transforms.ToTensor()`: 将图像转换为张量。
- `transforms.Normalize(mean, std)`: 归一化操作。


#### complete code
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
```

## 第三步：Create fully connected networks

```python
# 定义神经网络类：NN 类包含两个全连接层，并在前向传播中应用 ReLU 激活函数。
class NN(nn.Module):
    def __init__(self, input_size, num_classes):

        # 实例化模型：创建一个 NN 类的实例，指定输入大小和输出类别数量。
        super(NN, self).__init__()

        # defines the first fully connected layer with an input size of 784 and an output size of 50.
        self.fc1 = nn.Linear(input_size, 50)

        # defines the second fully connected layer with an input size of 50 and an output size of 10.
        self.fc2 = nn.Linear(50, num_classes)

    def forward(self, x):

        # The input x of shape [64, 784] is passed through self.fc1
        # resulting in an intermediate tensor of shape [64, 50].
        # The ReLU activation function is applied, but this does not change the shape
        # so the intermediate tensor remains [64, 50].
        x = F.relu(self.fc1(x))  # 应用 ReLU 激活函数

        # The intermediate tensor of shape [64, 50] is passed through self.fc2
        # resulting in an output tensor of shape [64, 10].
        x = self.fc2(x)

        return x
```

设置一个断点来检查模型的大小是否正确

```python
# Create a sample network to verify
Model = NN(784, 10)
x = torch.randn(64, 784)
print(Model(x).shape)
```

## 第四步：Set device

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

## 第五步：Hyperparameters

```python
input_size = 784
num_classes = 10
learning_rate = 0.01
batch_size = 64
num_epochs = 1
```

## 第六步：Load data

```python
# 加载训练数据集并将图像转换为张量
train_dataset = datasets.MNIST(root='dataset/',train=True, transform=transforms.ToTensor(), download=True)
# 创建训练数据加载器，每个批次大小为 batch_size，并在每个 epoch 开始前打乱数据
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
# 加载测试数据集并将图像转换为张量
test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)
# 创建测试数据加载器，每个批次大小为 batch_size，并在每个 epoch 开始前打乱数据
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)
```

## 第七步：Initialize network

```python
model = NN(input_size=input_size, num_classes=num_classes).to(device)
```

## 第八步：Loss and optimizer

```python
# criterion 是损失函数，用于计算模型输出与真实标签之间的误差。
# nn.CrossEntropyLoss()：这是交叉熵损失函数，通常用于多分类问题。
# 它将模型的输出（未归一化的得分）和目标标签作为输入，并计算损失值。
# 这个损失函数会自动应用 softmax 激活函数，因此不需要在输出层手动添加 softmax。
criterion = nn.CrossEntropyLoss()

# optimizer 是优化器，用于更新模型的参数，以最小化损失函数的值。
# optim.Adam：这是 Adam 优化器，一种常用的优化算法，结合了动量和自适应学习率调整的优点。
# model.parameters()：传递模型的参数给优化器，以便它可以更新这些参数。
# lr=learning_rate：设置学习率，决定每次更新模型参数时步长的大小。
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
```

## 第九步：Train network

```python
for epoch in range(num_epochs):
    for batch_index, (data,target) in enumerate (train_loader):
        # Get data to cuda if possible
        data = data.to(device=device)
        target = target.to(device=device)

        # Get to correct shape
        data = data.reshape(data.shape[0], -1)

        # forward
        scores = model(data)
        loss = criterion(scores, target)

        # backward
        optimizer.zero_grad()
        loss.backward()

        # gradient descent on adam step
        # 参数更新
        optimizer.step()
```

## 第十步：Check accuracy on training & test to see how good our model

```python
def check_accuracy(loader, model):

    if loader.dataset.train:
        print('Checking NN accuracy on training set')
    else:
        print('Checking NN accuracy on test set')

    num_correct = 0
    num_samples = 0

    # model.eval()：将模型设置为评估模式，禁用 dropout 和 batch normalization。
    model.eval()

    # torch.no_grad()：在这个上下文中禁用梯度计算，以节省内存和计算资源。
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device=device)
            y = y.to(device=device)
            x = x.reshape(x.shape[0], -1)

            scores = model(x)
            _, predictions = scores.max(1)
            num_correct += (predictions == y).sum()
            num_samples += predictions.size(0)

        print(f'Got {num_correct} / {num_samples} with NN accuracy {(num_correct / num_samples) * 100:.2f}')

    model.train()

check_accuracy(train_loader, model)
check_accuracy(test_loader, model)
```

## PyToch Output


```python
Checking NN accuracy on training set
Got 57110 / 60000 with NN accuracy 95.18

Checking NN accuracy on test set
Got 9487 / 10000 with NN accuracy 94.87
```


## Complete Code

```python
# Imports
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Create fully connected networks
class NN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(NN, self).__init__()
        self.fc1 = nn.Linear(input_size, 50)
        self.fc2 = nn.Linear(50, num_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Create a sample network to verify
# Model = NN(784, 10)
# x = torch.randn(64, 784)
# print(Model(x).shape)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
input_size = 784
num_classes = 10
learning_rate = 0.01
batch_size = 64
num_epochs = 1

# Load data
train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)

# Initialize network
model = NN(input_size=input_size, num_classes=num_classes).to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),lr =learning_rate)

# Train network
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        # Get data to cuda if possible
        data = data.to(device=device)
        target = target.to(device=device)

        # Get to correct shape
        data = data.reshape(data.shape[0],-1)

        # Forward
        output = model(data)
        loss = criterion(output, target)

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # Gradient descent or Adam step
        optimizer.step()

# Check accuracy on training & test to see how good our model

def check_accuracy(loader, model):

    if loader.dataset == train_dataset:
        print("Checking NN accuracy on training set")
    else:
        print("Checking NN accuracy on test set")

    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        for data, target in loader:
            data = data.to(device=device)
            target = target.to(device=device)

            data = data.reshape(data.shape[0],-1)

            output = model(data)
            _,predictions = output.max(dim=1)

            num_correct += (predictions == target).sum()
            num_samples += predictions.size(0)

        print(f'Got {num_correct} / {num_samples} with NN accuracy {num_correct / num_samples * 100:.2f}')

    model.train()

check_accuracy(train_loader, model)
check_accuracy(test_loader, model)
```

