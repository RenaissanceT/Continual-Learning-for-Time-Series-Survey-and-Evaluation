# Recurrent Neural Network Benchmark 

## 第一步：搭建框架

```python
# Imports

# Create fully connected networks

# Set device

# Hyperparameters

# Load data

# Initialize network

# Loss and optimizer

# Train network

# Check accuracy on training & test to see how good our model

```


## 第二步：Imports

#### 1. 
```python
import torch
```
torch 是 PyTorch 的核心库，它提供了多种张量操作和自动求导功能。

#### 2.  
```python
import torch.nn as nn
```
`torch.nn` : 包含了构建神经网络的基本组件，例如各种层和损失函数。
- `nn.Module`: 所有神经网络模块的基类。
- `nn.Linear`: 全连接层。
- `nn.Conv2d`: 卷积层。
- `nn.ReLU`: 激活函数。

#### 3.  
```python
import torch.optim as optim
```

`torch.optim` 包含了各种优化算法，用于更新神经网络的参数以最小化损失函数。

- `optim.SGD`: 随机梯度下降优化器。
- `optim.Adam`: Adam 优化器。

#### 4.  
```python
import torch.nn.functional as F
```
`torch.nn.functional` 提供了函数式的接口来执行神经网络的各种操作和计算，例如激活函数、卷积操作等。
- `F.relu`: ReLU 激活函数。
- `F.softmax`: Softmax 函数。

#### 5.  
```python
from torch.utils.data import DataLoader
```

`DataLoader` 是 PyTorch 提供的数据加载工具，用于将数据集加载到内存中并批量化处理。

- `DataLoader`: 数据加载器，负责打乱数据、分批次加载。

#### 6.  
```python
import torchvision.datasets as datasets
```

`torchvision.datasets` 包含了许多常用的公共数据集，可以直接用于训练和测试模型。
- (Create a binibatch to test on)
- `datasets.MNIST`: MNIST 手写数字数据集。
- `datasets.CIFAR10`: CIFAR-10 图像数据集。

#### 7.  
```python
import torchvision.transforms as transforms
```
`torchvision.transforms` 提供了一些常用的数据预处理和增强操作。
- `transforms.ToTensor()`: 将图像转换为张量。
- `transforms.Normalize(mean, std)`: 归一化操作。


#### complete code
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
```

## 第三步：Create fully connected networks

```python
class NN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(NN, self).__init__()
        self.fc1 = nn.Linear(input_size, 50)
        self.fc2 = nn.Linear(50, num_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

设置一个断点来检查模型的大小是否正确

```python
# Create a sample network to verify
Model = NN(784, 10)
x = torch.randn(64, 784)
print(Model(x).shape)
```

## 第四步：Create a simple Recurrent Neural Network


```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)

        # self.gru = nn.GRU(input_size, hidden_size, num_layer, batch_first=True)
        # self.lstm = nn.LSTM(hidden_size, hidden_size, num_layer, batch_first=True)

        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)

    def forward (self, x):
        # creates a tensor of zeros with shape (num_layers, batch_size, hidden_size)
        # moves the tensor to the specified device (CPU or GPU).
        h0 = torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)

        # Forward Propagation through RNN
        # The input x and the initial hidden state h0 are passed through the RNN.
        # The RNN outputs out (the output features for all time steps)
        # _ (the hidden state for the last time step, which we don't use here).

        # out, _ = self.gru(x, h0)
        # out, _ = self.lstm(x, (h0, c0))
        out, _ = self.rnn(x, h0)

        # out is reshaped to collapse the sequence dimension, resulting in a 2D tensor of shape
        out = out.reshape(out.shape[0], -1)
        out = self.fc(out)

        # out = self.fc(out[:, -1, :])
        return out

# Summary
# This class defines a simple RNN model with the following steps:
#
# 1. Initialize the RNN and a fully connected layer.

# 2. In the forward pass:
#    Initialize the hidden state.
#    Pass the input and initial hidden state through the RNN.
#    Reshape the RNN output.
#    Pass the reshaped output through the fully connected layer to get the final class scores.
```
## 第五步：Set device

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

## 第六步：Hyperparameters

在 RNN 里参数如下：
```python
input_size = 28
sequence_length = 28
num_layers = 2
hidden_layer = 256

num_classes = 10
learning_rate = 0.001
batch_size = 64
num_epochs = 2
```

在 NN / CNN 里参数如下:
```python
input_size = 784
num_classes = 10
learning_rate = 0.01
batch_size = 64
num_epochs = 1
```

## 第七步：Load data

```python
# 加载训练数据集并将图像转换为张量
train_dataset = datasets.MNIST(root='dataset/',train=True, transform=transforms.ToTensor(), download=True)
# 创建训练数据加载器，每个批次大小为 batch_size，并在每个 epoch 开始前打乱数据
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
# 加载测试数据集并将图像转换为张量
test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)
# 创建测试数据加载器，每个批次大小为 batch_size，并在每个 epoch 开始前打乱数据
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)
```

## 第八步：Initialize network

- 在 RNN 里面是
  
```python
model = RNN(input_size, hidden_layer, num_layers, num_classes).to(device)
```

- 在 CNN 里面是
  
```python
model = CNN().to(device)
```

- 在 NN 里面是
  
```python
model = NN(input_size=input_size, num_classes=num_classes).to(device)
```

## 第九步：Loss and optimizer

```python
# criterion 是损失函数，用于计算模型输出与真实标签之间的误差。
# nn.CrossEntropyLoss()：这是交叉熵损失函数，通常用于多分类问题。
# 它将模型的输出（未归一化的得分）和目标标签作为输入，并计算损失值。
# 这个损失函数会自动应用 softmax 激活函数，因此不需要在输出层手动添加 softmax。
criterion = nn.CrossEntropyLoss()

# optimizer 是优化器，用于更新模型的参数，以最小化损失函数的值。
# optim.Adam：这是 Adam 优化器，一种常用的优化算法，结合了动量和自适应学习率调整的优点。
# model.parameters()：传递模型的参数给优化器，以便它可以更新这些参数。
# lr=learning_rate：设置学习率，决定每次更新模型参数时步长的大小。
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
```

## 第十步：Train network

#### 请注意，在 CNN / RNN 里面不需要数据重塑。故而，

```python
        # Get to correct shape
        # 训练数据时，你将数据重塑为 (batch_size, -1)
        # 但这是为全连接网络设计的，卷积神经网络不需要这个步骤。
        # data = data.reshape(data.shape[0],-1)
```

#### 请注意，在 RNN

```python
            data = data.to(device=device).squeeze(1)
````

#### complete code

```python
for epoch in range(num_epochs):
    for batch_index, (data,target) in enumerate (train_loader):
        # Get data to cuda if possible
        data = data.to(device=device).squeeze(1)
        target = target.to(device=device)

        # Get to correct shape
        # 训练数据时，你将数据重塑为 (batch_size, -1)
        # 但这是为全连接网络设计的，卷积神经网络不需要这个步骤。
        # data = data.reshape(data.shape[0],-1)

        # forward
        scores = model(data)
        loss = criterion(scores, target)

        # backward
        optimizer.zero_grad()
        loss.backward()

        # gradient descent on adam step
        # 参数更新
        optimizer.step()
```

## 第十一步：Check accuracy on training & test to see how good our model


#### 请注意，在 CNN / RNN 里面不需要数据重塑。故而，

```python
        # Get to correct shape
        # 训练数据时，你将数据重塑为 (batch_size, -1)
        # 但这是为全连接网络设计的，卷积神经网络不需要这个步骤。
        # data = data.reshape(data.shape[0],-1)
```

#### 请注意，在 RNN

```python
            data = data.to(device=device).squeeze(1)
````

#### complete code
```python
def check_accuracy(loader, model):

    if loader.dataset == train_dataset:
        print("Checking RNN accuracy on training set")
    else:
        print("Checking RNN accuracy on test set")

    num_correct = 0
    num_samples = 0
    # model.eval()：将模型设置为评估模式，禁用 dropout 和 batch normalization。
    model.eval()

    # torch.no_grad()：在这个上下文中禁用梯度计算，以节省内存和计算资源。
    with torch.no_grad():
        for data, target in loader:
            data = data.to(device=device).squeeze(1)
            target = target.to(device=device)

            output = model(data)
            _,predictions = output.max(dim=1)

            num_correct += (predictions == target).sum()
            num_samples += predictions.size(0)

        print(f'Got {num_correct} / {num_samples} with RNN accuracy {num_correct / num_samples * 100:.2f}')

    model.train()

check_accuracy(train_loader, model)
check_accuracy(test_loader, model)
```

## PyToch Output

```python
Checking RNN accuracy on training set
Got 58527 / 60000 with RNN accuracy 97.55

Checking RNN accuracy on test set
Got 9746 / 10000 with RNN accuracy 97.46
```

## Complete Code

```python
# Imports
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from setuptools.dist import sequence
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Todo: Create a simple Recurrent Neural Network
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)

        # self.gru = nn.GRU(input_size, hidden_size, num_layer, batch_first=True)
        # self.lstm = nn.LSTM(hidden_size, hidden_size, num_layer, batch_first=True)

        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)

    def forward (self, x):
        # creates a tensor of zeros with shape (num_layers, batch_size, hidden_size)
        # moves the tensor to the specified device (CPU or GPU).
        h0 = torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0),self.hidden_size).to(device)

        # Forward Propagation through RNN
        # The input x and the initial hidden state h0 are passed through the RNN.
        # The RNN outputs out (the output features for all time steps)
        # _ (the hidden state for the last time step, which we don't use here).

        # out, _ = self.gru(x, h0)
        # out, _ = self.lstm(x, (h0, c0))
        out, _ = self.rnn(x, h0)

        # out is reshaped to collapse the sequence dimension, resulting in a 2D tensor of shape
        out = out.reshape(out.shape[0], -1)
        out = self.fc(out)

        # out = self.fc(out[:, -1, :])
        return out

# Summary
# This class defines a simple RNN model with the following steps:
#
# 1. Initialize the RNN and a fully connected layer.

# 2. In the forward pass:
#    Initialize the hidden state.
#    Pass the input and initial hidden state through the RNN.
#    Reshape the RNN output.
#    Pass the reshaped output through the fully connected layer to get the final class scores.


# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
input_size = 28
sequence_length = 28
num_layers = 2
hidden_layer = 256

num_classes = 10
learning_rate = 0.001
batch_size = 64
num_epochs = 2

# Load data
train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)

# Initialize network
model = RNN(input_size, hidden_layer, num_layers, num_classes).to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),lr =learning_rate)

# Train network
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        # Get data to cuda if possible
        data = data.to(device=device).squeeze(1)
        target = target.to(device=device)

        # Forward
        output = model(data)
        loss = criterion(output, target)

        # Backward
        optimizer.zero_grad()
        loss.backward()

        # Gradient descent or Adam step
        optimizer.step()

# Check accuracy on training & test to see how good our model

def check_accuracy(loader, model):

    if loader.dataset == train_dataset:
        print("Checking RNN accuracy on training set")
    else:
        print("Checking RNN accuracy on test set")

    num_correct = 0
    num_samples = 0
    model.eval()

    with torch.no_grad():
        for data, target in loader:
            data = data.to(device=device).squeeze(1)
            target = target.to(device=device)

            output = model(data)
            _,predictions = output.max(dim=1)

            num_correct += (predictions == target).sum()
            num_samples += predictions.size(0)

        print(f'Got {num_correct} / {num_samples} with RNN accuracy {num_correct / num_samples * 100:.2f}')

    model.train()

check_accuracy(train_loader, model)
check_accuracy(test_loader, model)
```


