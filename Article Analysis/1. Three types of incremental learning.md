# [Three types of incremental learning](https://www.nature.com/articles/s42256-022-00568-3)

## 思维导图

## 1. Continual Learning（持续学习）
- **定义**: 持续学习是一种从非平稳数据流中逐步学习新信息的能力。
- **挑战**: 深度神经网络容易遗忘已学到的知识，称为“灾难性遗忘”。
- **目标**: 缩小人工智能与自然智能之间在增量学习能力方面的差距。
- **应用领域**: 医疗诊断、自动驾驶、金融市场预测等。

## 2. 三种持续学习场景
### 2.1 Task-Incremental Learning (Task-IL)
- **定义**: 算法必须逐步学习一组明确可区分的任务，并且在测试时任务的身份是已知的。
- **挑战**: 共享表示、计算复杂度优化、正向或反向迁移学习。
- **示例**: 学习不同的运动或乐器。

### 2.2 Domain-Incremental Learning (Domain-IL)
- **定义**: 算法需要在相同问题的不同上下文中进行学习，测试时任务身份未知。
- **挑战**: 输入分布随时间变化，不允许任务特定组件。
- **示例**: 识别不同照明条件下的对象、不同天气下的驾驶。

### 2.3 Class-Incremental Learning (Class-IL)
- **定义**: 算法必须逐步学习识别不断增加的对象或类别数量。
- **挑战**: 在训练过程中不同时观察到的类别之间进行区分。
- **示例**: 学习区分不同动物类别。

## 3. 实验比较与结果
- **实验场景**: 使用了Split MNIST和Split CIFAR-100进行对比实验。
- **比较方法**:
  - **Context-Specific Components (任务特定组件)**
    - 例如: XdG, Separate Networks
  - **Parameter Regularization (参数正则化)**
    - 例如: EWC, SI
  - **Functional Regularization (功能正则化)**
    - 例如: LwF, FROMP
  - **Replay (回放机制)**
    - 例如: DGR, BI-R, ER, A-GEM
  - **Template-based Classification (模板分类)**
    - 例如: iCaRL, Generative Classifier
- **实验结果总结**:
  - Task-IL > Domain-IL > Class-IL 难度依次增加。
  - **Parameter Regularization** 在Task-IL表现良好，但在Class-IL失败。
  - **Replay方法** 在所有场景中表现良好，特别是在Class-IL中。

## 4. 理论与实际场景的区别
- **学术设置**: 非重叠、基于分类的连续学习场景有助于单独研究每个场景的挑战。
- **实际应用**: 数据的非平稳性更加复杂，往往是不同学习场景的混合。

## 5. 持续学习的策略
- **Context-specific Components**: 针对每个上下文使用不同的网络部分。
- **Parameter Regularization**: 限制对关键参数的更改。
- **Functional Regularization**: 在锚点上保持输入输出映射。
- **Replay**: 通过回放过去数据代表当前学习过程。
- **Template-based Classification**: 学习每个类的模板，并基于最适合样本的模板进行分类。

## 6. 未来展望与建议
- **进一步研究领域**: 更加关注Domain-IL场景，提供更真实的上下文集。
- **混合学习问题**: 在真实世界中，持续学习问题通常是三种场景的组合。
- **持续学习场景的扩展应用**: 无监督学习、强化学习等。
