# [Three types of incremental learning](https://www.nature.com/articles/s42256-022-00568-3)

## 持续学习的基础概念

## 1. Continual Learning（持续学习）
- **定义**: 持续学习是一种从非平稳数据流中逐步学习新信息的能力。
- **挑战**: 深度神经网络容易遗忘已学到的知识，称为“灾难性遗忘”。
- **目标**: 缩小人工智能与自然智能之间在增量学习能力方面的差距。
- **应用领域**: 医疗诊断、自动驾驶、金融市场预测等。

## 2. 三种持续学习场景
### 2.1 Task-Incremental Learning (Task-IL)
- **定义**: 算法必须逐步学习一组明确可区分的任务，并且在测试时任务的身份是已知的。
- **挑战**: 共享表示、计算复杂度优化、正向或反向迁移学习。
- **示例**: 学习不同的运动或乐器。

### 2.2 Domain-Incremental Learning (Domain-IL)
- **定义**: 算法需要在相同问题的不同上下文中进行学习，测试时任务身份未知。
- **挑战**: 输入分布随时间变化，不允许任务特定组件。
- **示例**: 识别不同照明条件下的对象、不同天气下的驾驶。

### 2.3 Class-Incremental Learning (Class-IL)
- **定义**: 算法必须逐步学习识别不断增加的对象或类别数量。
- **挑战**: 在训练过程中不同时观察到的类别之间进行区分。
- **示例**: 学习区分不同动物类别。

## 3. 实验比较与结果
- **实验场景**: 使用了Split MNIST和Split CIFAR-100进行对比实验。
- **比较方法**:
  - **Context-Specific Components (任务特定组件)**
    - 针对每个上下文使用不同的网络部分
    - 例如: XdG, Separate Networks
  - **Parameter Regularization (参数正则化)**
    - 限制对关键参数的更改
    - 例如: EWC, SI
  - **Functional Regularization (功能正则化)**
    - 在锚点上保持输入输出映射
    - 例如: LwF, FROMP
  - **Replay (回放机制)**
    - 通过回放过去数据代表当前学习过程
    - 例如: DGR, BI-R, ER, A-GEM
  - **Template-based Classification (模板分类)**
    - 学习每个类的模板，并基于最适合样本的模板进行分类
    - 例如: iCaRL, Generative Classifier
- **实验结果总结**:
  - **Task-IL** > **Domain-IL** > **Class-IL** 难度依次增加。
  - **Parameter Regularization** 在 Task-IL 表现良好，但在 Class-IL 失败。
  - **Replay**方法 在所有场景中表现良好，特别是在 Class-IL 中。



