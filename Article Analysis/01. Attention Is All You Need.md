# [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

<img width="987" alt="Screen Shot 2024-09-29 at 6 08 47 PM" src="https://github.com/user-attachments/assets/5e306cf1-26c2-492d-8762-7457b06d993b">

----

# 核心要点

----

<img src="https://github.com/user-attachments/assets/cf2efb15-3fcb-418e-99a5-7ea2a125abaf" width="100%" height="100%">

---

<img src="https://github.com/user-attachments/assets/fcd88f68-d644-4d71-8e8f-5ad98b240560" width="49%" height="10%">

<img src="https://github.com/user-attachments/assets/a2927df2-71f2-47a1-b6e3-af692c7d71fd" width="49%" height="100%">

---
## CNN、RNN 和 Self-Attention 的复杂度区别

<img width="987" alt="Screen Shot 2024-09-29 at 6 15 58 PM" src="https://github.com/user-attachments/assets/f70619be-13e7-4f52-8fcd-258f42761ada">

### [CNN、RNN 和 Self-Attention 的区别](https://github.com/RenaissanceT/Continual_Learning_for_Time_Series_Survey_and_Evaluation/blob/main/Project%20Progress/5.%20CNN%E3%80%81RNN%20%E5%92%8C%20Self-Attention%20%E7%9A%84%E5%8C%BA%E5%88%AB.md)

### 多头注意力机制 Multi-Head Attention 的复杂度是多少？详细的计算过程

**通俗解释**：多头注意力机制将注意力计算过程重复多次（多个头），然后将这些结果组合在一起。它能同时关注输入的不同方面，得到更丰富的信息。

**计算复杂度**：
- **假设**：输入序列长度为`n`，每个向量的维度为`d`，有`h`个注意力头。

1. **查询（Q）、键（K）、值（V）变换**：
   - 每个头的维度是`d/h`，计算线性变换的复杂度为 `O(n * d^2)`（因为需要对`Q`、`K`、`V`分别进行线性变换）。

2. **注意力计算**：
   - 每个头需要计算`QK^T`，这个矩阵乘法的复杂度是 `O(n^2 * d/h)`。

3. **所有头的计算**：
   - 有`h`个头，所以总的复杂度为 `h * O(n^2 * d/h) = O(n^2 * d)`。

4. **最终组合**：
   - 将所有头的输出拼接并线性变换，复杂度为 `O(n * d^2)`。

**总复杂度**：`O(n * d^2 + n^2 * d)`

**例子**：对于一个长句子，计算每个单词和其他单词的关系，多头注意力能同时计算多个视角，但由于序列很长（n大），复杂度主要体现在`n^2`部分。
