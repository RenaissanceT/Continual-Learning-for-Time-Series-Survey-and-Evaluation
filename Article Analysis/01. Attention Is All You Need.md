# [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

<img width="987" alt="Screen Shot 2024-09-29 at 6 08 47 PM" src="https://github.com/user-attachments/assets/5e306cf1-26c2-492d-8762-7457b06d993b">

----

# 核心要点

----

<img src="https://github.com/user-attachments/assets/cf2efb15-3fcb-418e-99a5-7ea2a125abaf" width="100%" height="100%">

---

<img src="https://github.com/user-attachments/assets/fcd88f68-d644-4d71-8e8f-5ad98b240560" width="49%" height="10%">

<img src="https://github.com/user-attachments/assets/a2927df2-71f2-47a1-b6e3-af692c7d71fd" width="49%" height="100%">

---
## 1. CNN、RNN 和 Self-Attention 的复杂度区别

<img width="987" alt="Screen Shot 2024-09-29 at 6 15 58 PM" src="https://github.com/user-attachments/assets/f70619be-13e7-4f52-8fcd-258f42761ada">

### [CNN、RNN 和 Self-Attention 的区别](https://github.com/RenaissanceT/Continual_Learning_for_Time_Series_Survey_and_Evaluation/blob/main/Project%20Progress/5.%20CNN%E3%80%81RNN%20%E5%92%8C%20Self-Attention%20%E7%9A%84%E5%8C%BA%E5%88%AB.md)

### 多头注意力机制 Multi-Head Attention 的复杂度是多少？详细的计算过程

**通俗解释**：多头注意力机制将注意力计算过程重复多次（多个头），然后将这些结果组合在一起。它能同时关注输入的不同方面，得到更丰富的信息。

**计算复杂度**：
- **假设**：输入序列长度为`n`，每个向量的维度为`d`，有`h`个注意力头。

1. **查询（Q）、键（K）、值（V）变换**：
   - 每个头的维度是`d/h`，计算线性变换的复杂度为 `O(n * d^2)`（因为需要对`Q`、`K`、`V`分别进行线性变换）。

2. **注意力计算**：
   - 每个头需要计算`QK^T`，这个矩阵乘法的复杂度是 `O(n^2 * d/h)`。

3. **所有头的计算**：
   - 有`h`个头，所以总的复杂度为 `h * O(n^2 * d/h) = O(n^2 * d)`。

4. **最终组合**：
   - 将所有头的输出拼接并线性变换，复杂度为 `O(n * d^2)`。

**总复杂度**：`O(n * d^2 + n^2 * d)`

**例子**：对于一个长句子，计算每个单词和其他单词的关系，多头注意力能同时计算多个视角，但由于序列很长（n大），复杂度主要体现在`n^2`部分。

----
## 2. BatchNorm 和 LayerNorm 的区别

Batch Normalization (BatchNorm) 和 Layer Normalization (LayerNorm) 都是深度学习中用于加速模型训练和提高稳定性的技巧，它们的目标是对神经网络层的输出进行归一化处理，但应用方式和适用场景有所不同。

### 1. **Batch Normalization (批归一化)**

**目的：**
- 归一化一个 mini-batch 内的样本，确保每个 mini-batch 的输出有相同的分布。主要用于卷积神经网络（CNN）中。

**工作方式：**
- 对每个 mini-batch 进行归一化，即对这个 batch 中每个特征的均值和方差进行计算，并通过这些统计量将输出归一化。
- 它会将数据调整为均值为 0，方差为 1 的分布，并学到缩放和偏移参数来调整数据。

**举例：**
- 假设有一个 2D 图像，每个图像有3个通道（例如RGB），一个 batch 包含 32 个图像。BatchNorm 会对这 32 个图像在每个通道上的特征进行归一化。例如，R 通道的像素值会根据该通道的均值和方差归一化。

**缺点：**
- 由于每次对 mini-batch 重新计算统计信息，在小 batch size 或测试阶段（单个样本）时，效果不太稳定。

**应用场景：**
- 适合 CNN，因为它针对整个 batch 的输入数据进行归一化，能够很好地处理不同样本间的变化。

### 2. **Layer Normalization (层归一化)**

**目的：**
- 归一化每一个样本的所有神经元的输出，独立于 mini-batch 的大小。通常用于循环神经网络（RNN）或 Transformer 中。

**工作方式：**
- LayerNorm 对每个样本的所有特征进行归一化，而不是对整个 batch 的特征进行归一化。它在每个样本的特征维度上计算均值和方差，因此不依赖 batch size。

**举例：**
- 假设你有一个文本句子，每个句子通过嵌入（embedding）后被表示为 10 个特征，LayerNorm 会对每个句子的这 10 个特征值进行归一化。

**优点：**
- 对 mini-batch 的大小不敏感，因此在小 batch size 或仅使用一个样本时，效果稳定。

**应用场景：**
- 通常用于 RNN 或 Transformer 模型中，这些模型每个时间步的输入具有不同特征维度，LayerNorm 更适合这种情况。

### **总结：**

- **BatchNorm** 归一化的是整个 mini-batch 中的特征，适合卷积神经网络，尤其在大 batch 训练时表现优越。
- **LayerNorm** 归一化的是单个样本的所有特征，不依赖 mini-batch 大小，适合 RNN 或 Transformer 这种结构。

### **小例子：**

假设你有一个 3 个样本的 batch，每个样本有 3 个特征值：

```python
样本1: [1, 2, 3]
样本2: [4, 5, 6]
样本3: [7, 8, 9]
```

- **BatchNorm**：计算每个特征的均值和方差（例如第1个特征是[1, 4, 7]），然后对每个特征进行归一化，结果是每个特征在整个 batch 中被归一化。

- **LayerNorm**：对每个样本的所有特征值计算均值和方差（例如样本1的均值是2，方差是1），然后对该样本的特征进行归一化。

----

## 3. [Transformer 基础概念](https://github.com/RenaissanceT/Continual_Learning_for_Time_Series_Survey_and_Evaluation/blob/main/Project%20Progress/7.%20Transformer%20%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md)

### Transformer 的 Attention 机制

**通俗解释**：Transformer的Attention机制是一种帮助模型“关注”输入序列中重要部分的技术。它通过计算序列中各个部分之间的相关性，找到哪些部分对当前任务最有用，并给予它们更高的权重。

**例子**：假设我们有一个英语句子“Cats like to sleep”。如果要翻译成中文，Transformer会关注每个单词和其他单词的关系，找出“Cats”和“sleep”之间的联系，从而更好地翻译句子。

**详细过程**：
1. 输入序列经过查询（Query）、键（Key）和值（Value）的线性变换。
2. 通过查询和键的点积，计算每个单词之间的相关性。
3. 使用Softmax函数，将相关性转换为概率，得到注意力权重。
4. 将这些注意力权重与值相乘，得到最终的输出。

这样，Attention机制就能够灵活地选择序列中最相关的信息，避免只依赖固定位置的上下文。

# 论文结论



