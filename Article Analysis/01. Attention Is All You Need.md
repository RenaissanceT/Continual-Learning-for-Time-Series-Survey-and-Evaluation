# [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

<img width="987" alt="Screen Shot 2024-09-29 at 6 08 47 PM" src="https://github.com/user-attachments/assets/5e306cf1-26c2-492d-8762-7457b06d993b">

----

# 核心要点

----

<img src="https://github.com/user-attachments/assets/cf2efb15-3fcb-418e-99a5-7ea2a125abaf" width="100%" height="100%">

---

<img src="https://github.com/user-attachments/assets/fcd88f68-d644-4d71-8e8f-5ad98b240560" width="49%" height="10%">

<img src="https://github.com/user-attachments/assets/a2927df2-71f2-47a1-b6e3-af692c7d71fd" width="49%" height="100%">

---
## 1. CNN、RNN 和 Self-Attention 的复杂度区别

<img width="987" alt="Screen Shot 2024-09-29 at 6 15 58 PM" src="https://github.com/user-attachments/assets/f70619be-13e7-4f52-8fcd-258f42761ada">

### [CNN、RNN 和 Self-Attention 的区别](https://github.com/RenaissanceT/Continual_Learning_for_Time_Series_Survey_and_Evaluation/blob/main/Project%20Progress/5.%20CNN%E3%80%81RNN%20%E5%92%8C%20Self-Attention%20%E7%9A%84%E5%8C%BA%E5%88%AB.md)

### 多头注意力机制 Multi-Head Attention 的复杂度是多少？详细的计算过程

**通俗解释**：多头注意力机制将注意力计算过程重复多次（多个头），然后将这些结果组合在一起。它能同时关注输入的不同方面，得到更丰富的信息。

**计算复杂度**：
- **假设**：输入序列长度为`n`，每个向量的维度为`d`，有`h`个注意力头。

1. **查询（Q）、键（K）、值（V）变换**：
   - 每个头的维度是`d/h`，计算线性变换的复杂度为 `O(n * d^2)`（因为需要对`Q`、`K`、`V`分别进行线性变换）。

2. **注意力计算**：
   - 每个头需要计算`QK^T`，这个矩阵乘法的复杂度是 `O(n^2 * d/h)`。

3. **所有头的计算**：
   - 有`h`个头，所以总的复杂度为 `h * O(n^2 * d/h) = O(n^2 * d)`。

4. **最终组合**：
   - 将所有头的输出拼接并线性变换，复杂度为 `O(n * d^2)`。

**总复杂度**：`O(n * d^2 + n^2 * d)`

**例子**：对于一个长句子，计算每个单词和其他单词的关系，多头注意力能同时计算多个视角，但由于序列很长（n大），复杂度主要体现在`n^2`部分。

----
## 2. BatchNorm 和 LayerNorm 的区别

Batch Normalization (BatchNorm) 和 Layer Normalization (LayerNorm) 都是深度学习中用于加速模型训练和提高稳定性的技巧，它们的目标是对神经网络层的输出进行归一化处理，但应用方式和适用场景有所不同。

### 1. **Batch Normalization (批归一化)**

**目的：**
- 归一化一个 mini-batch 内的样本，确保每个 mini-batch 的输出有相同的分布。主要用于卷积神经网络（CNN）中。

**工作方式：**
- 对每个 mini-batch 进行归一化，即对这个 batch 中每个特征的均值和方差进行计算，并通过这些统计量将输出归一化。
- 它会将数据调整为均值为 0，方差为 1 的分布，并学到缩放和偏移参数来调整数据。

**举例：**
- 假设有一个 2D 图像，每个图像有3个通道（例如RGB），一个 batch 包含 32 个图像。BatchNorm 会对这 32 个图像在每个通道上的特征进行归一化。例如，R 通道的像素值会根据该通道的均值和方差归一化。

**缺点：**
- 由于每次对 mini-batch 重新计算统计信息，在小 batch size 或测试阶段（单个样本）时，效果不太稳定。

**应用场景：**
- 适合 CNN，因为它针对整个 batch 的输入数据进行归一化，能够很好地处理不同样本间的变化。

### 2. **Layer Normalization (层归一化)**

**目的：**
- 归一化每一个样本的所有神经元的输出，独立于 mini-batch 的大小。通常用于循环神经网络（RNN）或 Transformer 中。

**工作方式：**
- LayerNorm 对每个样本的所有特征进行归一化，而不是对整个 batch 的特征进行归一化。它在每个样本的特征维度上计算均值和方差，因此不依赖 batch size。

**举例：**
- 假设你有一个文本句子，每个句子通过嵌入（embedding）后被表示为 10 个特征，LayerNorm 会对每个句子的这 10 个特征值进行归一化。

**优点：**
- 对 mini-batch 的大小不敏感，因此在小 batch size 或仅使用一个样本时，效果稳定。

**应用场景：**
- 通常用于 RNN 或 Transformer 模型中，这些模型每个时间步的输入具有不同特征维度，LayerNorm 更适合这种情况。

### **总结：**

- **BatchNorm** 归一化的是整个 mini-batch 中的特征，适合卷积神经网络，尤其在大 batch 训练时表现优越。
- **LayerNorm** 归一化的是单个样本的所有特征，不依赖 mini-batch 大小，适合 RNN 或 Transformer 这种结构。

### **小例子：**

假设你有一个 3 个样本的 batch，每个样本有 3 个特征值：

```python
样本1: [1, 2, 3]
样本2: [4, 5, 6]
样本3: [7, 8, 9]
```

- **BatchNorm**：计算每个特征的均值和方差（例如第1个特征是[1, 4, 7]），然后对每个特征进行归一化，结果是每个特征在整个 batch 中被归一化。

- **LayerNorm**：对每个样本的所有特征值计算均值和方差（例如样本1的均值是2，方差是1），然后对该样本的特征进行归一化。

----

## 3. [Transformer 基础概念](https://github.com/RenaissanceT/Continual_Learning_for_Time_Series_Survey_and_Evaluation/blob/main/Project%20Progress/7.%20Transformer%20%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md)

### Transformer 的 Attention 机制

**通俗解释**：Transformer的Attention机制是一种帮助模型“关注”输入序列中重要部分的技术。它通过计算序列中各个部分之间的相关性，找到哪些部分对当前任务最有用，并给予它们更高的权重。

**例子**：假设我们有一个英语句子“Cats like to sleep”。如果要翻译成中文，Transformer会关注每个单词和其他单词的关系，找出“Cats”和“sleep”之间的联系，从而更好地翻译句子。

**详细过程**：
1. 输入序列经过查询（Query）、键（Key）和值（Value）的线性变换。
2. 通过查询和键的点积，计算每个单词之间的相关性。
3. 使用Softmax函数，将相关性转换为概率，得到注意力权重。
4. 将这些注意力权重与值相乘，得到最终的输出。

这样，Attention机制就能够灵活地选择序列中最相关的信息，避免只依赖固定位置的上下文。

---

# 精读笔记

## 1. Transformer架构中，为什么选择放弃RNN和CNN，仅使用注意力机制？
**答案：**
- Transformer摒弃了RNN和CNN，完全依赖自注意力机制，因为RNN在处理长序列时存在依赖关系传递过慢的问题，而CNN则难以捕捉远距离的依赖。自注意力机制允许模型直接捕捉序列中任意位置间的依赖关系，同时支持更高的并行性，从而大幅提高了训练效率。

**解释：**
RNN是逐步处理每个序列位置，导致长序列中前后信息的传递较慢。而Transformer的自注意力机制能够直接关注序列中任意两个位置的信息。例如，在翻译长句子时，RNN需要一步步记忆上下文，而Transformer可以立刻捕捉句子中远距离的词之间的关系。


## 2. 什么是多头注意力机制（Multi-Head Attention），它的作用是什么？
**答案：**
- 多头注意力机制是指模型在不同的子空间上并行地计算多组注意力（attention），然后将这些注意力结果拼接在一起。它的作用是让模型能够从多个角度捕捉序列中不同位置的相关性。

**解释：**
多头注意力可以理解为一个“团队工作”，每个“头”专注于不同的信息。比如，当翻译一句话时，一个头可能专注于主语和谓语的关系，而另一个头则可能专注于修饰语和名词的关系。这种机制让模型在一次计算中获取更多信息。


## 3. 什么是Scaled Dot-Product Attention，为什么要进行缩放？
**答案：**
- Scaled Dot-Product Attention通过计算“query”和“key”之间的点积，确定它们的相关性，并将结果通过Softmax函数得到注意力权重。缩放的原因是避免点积的数值过大，导致Softmax函数的梯度变得极小，从而影响模型的学习效果。

**解释：**
如果不进行缩放，随着“query”和“key”维度的增加，它们的点积值会变得很大，Softmax函数对这种大值敏感度较低，会产生非常小的梯度。缩放通过除以\(\sqrt{d_k}\)（即key的维度平方根）可以将点积控制在适当范围。例如，如果点积过大，模型会更难有效学习。


## 4. Transformer中使用了什么样的位置信息编码？为什么需要位置信息？
**答案：**
- Transformer使用正弦和余弦函数生成的**位置编码**（Positional Encoding），因为自注意力机制本身不包含序列顺序信息。位置信息帮助模型识别输入序列中不同位置之间的顺序关系。

**解释：**
RNN通过时间步长自然获取序列顺序信息，而Transformer没有序列依赖，因此必须显式添加位置信息。例如，在句子中，单词的顺序很重要。位置信息编码可以告诉模型“前面是主语，后面是谓语”，从而保持顺序的正确性。


## 5. 什么是残差连接（Residual Connection），为什么Transformer中使用它？
**答案：**
- 残差连接是指将输入直接传递到子层输出中，并与子层输出相加。它可以缓解深层神经网络中梯度消失的问题，从而让更深的网络更容易训练。

**解释：**
在深层网络中，信号逐层传递时会逐渐减弱，导致学习困难。残差连接像是一座“捷径桥”，让输入可以绕过某些层，直接传递到后面的层。例如，如果网络太深，前面的信号难以传递到后面，残差连接可以缓解这种问题。


## 6. 在Transformer中，编码器和解码器的作用分别是什么？
**答案：**
- **编码器**将输入序列（如源语言句子）转化为一系列连续表示（embeddings），**解码器**根据编码器的输出生成目标序列（如目标语言句子）。

**解释：**
编码器是输入的“理解者”，将输入转换为模型能够处理的表示。解码器则是“生成者”，根据编码器的结果生成输出。比如，在机器翻译中，编码器“理解”输入句子含义，而解码器则生成翻译后的句子。


## 7. 什么是自注意力机制（Self-Attention），它与传统注意力机制有何不同？
**答案：**
- 自注意力机制是指在一个序列内部，计算序列中每个位置和其他所有位置的相关性。传统的注意力机制通常用于序列间的操作，如编码器和解码器之间。

**解释：**
在机器翻译中，传统注意力机制通常用于对齐源语言和目标语言的词语，而自注意力机制则是在输入或输出序列的不同位置之间计算相关性。例如，句子中的每个单词可以与其他单词建立联系，而不需要跨序列的计算。


## 8. Transformer为什么能显著加速训练过程？
**答案：**
- Transformer摒弃了RNN的序列依赖，使用并行的自注意力机制，使得训练时可以同时处理序列中的每个位置。这显著提高了计算效率。

**解释：**
RNN必须按顺序处理每个时间步，但Transformer的并行处理让它可以一次性处理整个序列。例如，在处理长句子时，RNN需要逐字逐句处理，而Transformer可以一次性分析整个句子。


## 9. 在机器翻译任务中，Transformer模型是如何实现SOTA（State-of-the-Art）性能的？
**答案：**
- Transformer通过多头自注意力机制和位置编码，使模型能够捕捉到复杂的长距离依赖关系，且大幅提升了训练和推理速度。在WMT 2014英德和英法翻译任务中，Transformer超越了之前的所有模型，包括使用了集成模型的结果。

**解释：**
Transformer的自注意力机制让它可以同时关注句子中多个关键点，如主语和谓语、定语和名词之间的关系，这使得它比传统模型在翻译任务中表现更好。例如，它能够在一句复杂的德语句子中，准确识别出句子结构并进行正确翻译。


## 10. 你认为Transformer的架构可以推广到哪些其他领域？为什么？
**答案：**
- Transformer架构不仅适用于自然语言处理，还可以推广到图像处理、语音识别等领域，因为自注意力机制能够在任何需要捕捉全局依赖关系的任务中发挥作用。

**解释：**
在图像处理中，像素之间的远距离依赖关系也很重要，比如一张图像中的不同区域可能相互关联。Transformer可以通过自注意力机制分析图像中的全局特征。同样，在语音处理任务中，长时间的依赖也可以通过这种机制进行建模。例如，图像分类中，Transformer可以关注图片中的局部特征并关联到全局背景。




