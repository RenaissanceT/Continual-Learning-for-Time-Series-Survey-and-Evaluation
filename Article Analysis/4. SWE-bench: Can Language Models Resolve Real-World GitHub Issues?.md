# [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770)



### 8.7 通过时间反向传播（BPTT）是什么？
我们先理解一下什么是"通过时间反向传播"（Backpropagation Through Time，BPTT）。其实它是一种把反向传播应用到序列模型（比如循环神经网络 RNN）的方法。我们知道RNN是一种处理序列数据的模型，比如你要分析一段文本，RNN会一个词一个词地读取序列，依赖前面的内容来预测后面的内容。

要明白BPTT，我们先复习一下普通的反向传播是什么。普通的反向传播主要用于计算模型的误差，并调整参数（比如权重），让模型预测得更准确。而在RNN中，情况有点不同，因为模型需要处理一个序列的输入，所以我们就有了“通过时间”的部分，意思是我们要把整个序列展开，然后将误差一层层传递回去。

#### BPTT的基本过程
在RNN中，模型会在每一个时间步上处理数据，比如一个句子有100个单词，RNN就会有100个时间步，每一个时间步的输出会影响下一个时间步的输入。所以，当我们展开这个过程，就好像把这个RNN“拉直”成一个长链条，每个时间步都是其中的一个环节。

通过时间反向传播就是要把整个展开的链条从最后一个时间步开始，按照链式法则一步一步地计算每一层对前面时间步的影响，然后根据这些影响来调整每个参数的值。

### 梯度消失与梯度爆炸
在RNN的反向传播过程中，存在一个很严重的问题，那就是“梯度爆炸”或“梯度消失”。

- **梯度爆炸**：当梯度值随着时间步的增加不断累积，最后会变得非常大，导致参数更新时出现不稳定，甚至产生非常大的误差。
- **梯度消失**：与梯度爆炸相反，当梯度在每一层传播时逐渐变小，最终趋近于零，这样模型就无法继续学习。

为了避免这两个问题，通常会采取一些技术，比如“梯度截断”（Gradient Clipping）或使用特殊的模型结构，比如LSTM（长短期记忆网络）和GRU（门控循环单元）。

### 8.7.1 RNN中的梯度分析
我们来看一下如何分析RNN的梯度。RNN模型的输入、隐藏层状态、输出在每一个时间步上都彼此依赖。计算梯度的时候，会因为前面时间步的状态对后面时间步的影响而导致梯度的复杂性增加。你可以想象一下，如果序列很长，比如1000个字符，计算梯度时，你需要考虑到1000个时间步的影响，这会非常复杂，容易导致梯度消失或爆炸。

为了避免这个问题，我们使用一种方法叫做**截断**，我们只在一段时间内计算梯度，而不是让它一直延伸到整个序列的开头。

#### 梯度截断的三种策略
1. **完全计算**：这是最原始的方法，直接计算所有时间步上的梯度。这虽然最准确，但在实际中不可行，因为会遇到梯度爆炸或梯度消失的问题，而且计算成本非常高。

2. **固定步长截断**：这是我们最常用的方法。我们只计算某个固定步长内的梯度，然后截断梯度，不让它继续传递下去。这就相当于告诉模型只关心最近的一段时间步上的信息，而忽略更远的影响，这样可以有效地防止梯度消失或爆炸。

3. **随机截断**：随机截断则是每次随机地决定在某个时间步截断梯度。这种方法在理论上看起来不错，但在实践中却没有带来明显的优势，因此不如固定步长截断常用。

### 8.7.2 BPTT的细节
在这里，我们会更仔细地看一下BPTT是如何运作的。BPTT的核心思想就是把RNN的所有时间步展开成一个计算图，然后对每一个时间步上的参数和输出进行梯度计算。

你可以把这个过程想象成搭积木，我们先把积木一块一块搭起来（这是前向传播），然后再一块一块地拆下来（这是反向传播）。在这个拆的过程中，我们会对每一个积木计算它对整个模型的影响（梯度），并调整它的参数。

我们需要注意的是，在计算过程中，为了节省计算资源，我们会缓存中间的结果（比如每个时间步的隐状态和输出），以便在反向传播时可以直接使用这些结果，避免重复计算。

### 8.7.3 小结
- BPTT是反向传播在处理序列模型时的一种应用，它特别针对RNN这样的具有隐藏状态的模型。
- 为了避免梯度消失或爆炸，BPTT通常会使用“截断”技术，即只计算一段时间内的梯度。
- 在实践中，我们经常使用固定步长截断，因为它简单有效。

### 回答练习问题
1. **对称矩阵的特征值与特征向量**：假设我们有一个对称矩阵，它的特征值是\( \lambda_i \)，对应的特征向量是\( v_i \)，那么这个矩阵的幂（比如 \( W^t \)）的特征值就是\( \lambda_i^t \)。这意味着随着时间步数增加，特征值会以指数级增长或减小。

2. **随机向量与特征向量**：对于一个随机向量 \( x \)，当矩阵的幂 \( W^t \) 作用于 \( x \) 时，由于特征值 \( \lambda \) 的大小不同，最终较大的特征值会占据主导地位。因此，经过多次相乘，向量 \( x \) 会逐渐与最大的特征值对应的特征向量 \( v_i \) 在同一直线上。这种现象说明，长序列中，梯度可能会被某一个方向主导，导致梯度爆炸或消失。

3. **其他解决梯度爆炸的方法**：
   - **使用更稳定的激活函数**：比如ReLU或Leaky ReLU。
   - **使用LSTM或GRU**：它们具有特殊的结构，可以有效地缓解梯度消失和爆炸。
   - **正规化**：对权重进行正则化，防止它们变得太大。
   - **调整学习率**：选择合适的学习率，避免参数更新过大。

希望这个解释能帮你理解“通过时间反向传播”的原理。如果有不明白的地方，随时可以问我！
